{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd.functional import jacobian         # computation graph\n",
    "from torch import Tensor, nn, optim                 # tensor node in the computation graph\n",
    "# import torch.nn as nn                     # neural networks\n",
    "# import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "import time\n",
    "from scipy.integrate import simps\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the datasets from experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'exp_data'\n",
    "\n",
    "# List of all the folders in the base directory\n",
    "folders = [f for f in glob.glob(os.path.join(base_dir, '*')) if os.path.isdir(f)]\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for folder in folders:\n",
    "    # List of all the files in the folder\n",
    "    files = glob.glob(os.path.join(folder, '*.csv'))\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for file in files:\n",
    "        data.append(pd.read_csv(file))\n",
    "    \n",
    "    datasets[folder] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER Functions for course project\n",
    "\n",
    "def computeJacobian(F):\n",
    "    \"\"\"\n",
    "    Compute Jacobian from deformation gradient.\n",
    "    \n",
    "    _Input Arguments_\n",
    "    \n",
    "    - `F` - deformation gradient in Voigt notation\n",
    "    \n",
    "    _Output Arguments_\n",
    "    \n",
    "    - `J` - Jacobian\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    \"\"\"\n",
    "    F11 = F[:,0:1]\n",
    "    F12 = F[:,1:2]\n",
    "    F21 = F[:,2:3]\n",
    "    F22 = F[:,3:4]\n",
    "\n",
    "    J = F11*F22 - F12*F21\n",
    "    return J\n",
    "\n",
    "def computeCauchyGreenStrain(F):\n",
    "    \"\"\"\n",
    "    Compute right Cauchy-Green strain tensor from deformation gradient.\n",
    "    \n",
    "    _Input Arguments_\n",
    "    \n",
    "    - `F` - deformation gradient in Voigt notation\n",
    "    \n",
    "    _Output Arguments_\n",
    "    \n",
    "    - `C` - Cauchy-Green strain tensor in Voigt notation\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    \"\"\"\n",
    "    F11 = F[:,0:1]\n",
    "    F12 = F[:,1:2]\n",
    "    F21 = F[:,2:3]\n",
    "    F22 = F[:,3:4]\n",
    "\n",
    "    C11 = F11**2 + F21**2\n",
    "    C12 = F11*F12 + F21*F22\n",
    "    C21 = F11*F12 + F21*F22\n",
    "    C22 = F12**2 + F22**2\n",
    "\n",
    "    C = torch.cat((C11,C12,C21,C22),dim=1)\n",
    "    return C\n",
    "\n",
    "\n",
    "def computeStrainInvariants(C):\n",
    "    \"\"\"\n",
    "    Compute invariants of the Cauchy-Green strain tensor.\n",
    "    Plane strain is assumed.\n",
    "    \n",
    "    _Input Arguments_\n",
    "    \n",
    "    - `C` - Cauchy-Green strain tensor in Voigt notation\n",
    "    \n",
    "    _Output Arguments_\n",
    "    \n",
    "    - `I1` - 1st invariant\n",
    "    \n",
    "    - `I2` - 2nd invariant\n",
    "\n",
    "    - `I3` - 3rd invariant\n",
    "\n",
    "    ---\n",
    "    \n",
    "    \"\"\"\n",
    "    C11 = C[:,0:1]\n",
    "    C12 = C[:,1:2]\n",
    "    C21 = C[:,2:3]\n",
    "    C22 = C[:,3:4]\n",
    "\n",
    "    I1 = C11 + C22 + 1.0\n",
    "    I2 = C11 + C22 - C12*C21 + C11*C22\n",
    "    I3 = C11*C22 - C12*C21\n",
    "    return I1, I2, I3\n",
    "\n",
    "\n",
    "def computeStrainInvariantDerivatives(F,i,secondDerivative=False):\n",
    "    \"\"\"\n",
    "    Compute derivatives of the invariants of the Cauchy-Green strain tensor with respect to the deformation gradient.\n",
    "    Plane strain is assumed.\n",
    "    \n",
    "    _Input Arguments_\n",
    "    \n",
    "    - `F` - deformation gradient in Voigt notation\n",
    "\n",
    "    - `i` - specify the invariant that should be differentiated \n",
    "\n",
    "    - `secondDerivative` - specify if second derivative should be computed \n",
    "    \n",
    "    _Output Arguments_\n",
    "    \n",
    "    - `dIdF` - derivative (note that the size of `dIdF` depends on the choice of `secondDerivative`)\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    \"\"\"\n",
    "    F11 = F[:,0:1]\n",
    "    F12 = F[:,1:2]\n",
    "    F21 = F[:,2:3]\n",
    "    F22 = F[:,3:4]\n",
    "    if not(secondDerivative):\n",
    "        dIdF = torch.zeros(F.shape[0],F.shape[1])\n",
    "        if(i==1):\n",
    "            # dI1/dF:\n",
    "            dIdF = 2.0*F\n",
    "        elif(i==2):\n",
    "            # dI2/dF:\n",
    "            dIdF11 = 2.0*F11 - 2.0*F12*F21*F22 + 2.0*F11*(F22**2)\n",
    "            dIdF12 = 2.0*F12 + 2.0*F12*(F21**2) - 2.0*F11*F21*F22\n",
    "            dIdF21 = 2.0*F21 + 2.0*(F12**2)*F21 - 2.0*F11*F12*F22\n",
    "            dIdF22 = 2.0*F22 - 2.0*F11*F12*F21 + 2.0*(F11**2)*F22\n",
    "            dIdF = torch.cat((dIdF11,dIdF12,dIdF21,dIdF22),dim=1)\n",
    "        elif(i==3):\n",
    "            # dI3/dF:\n",
    "            J = F11*F22 - F12*F21\n",
    "            dIdF11 = 2.0*F22 * J\n",
    "            dIdF12 = -2.0*F21 * J\n",
    "            dIdF21 = -2.0*F12 * J\n",
    "            dIdF22 = 2.0*F11 * J\n",
    "            dIdF = torch.cat((dIdF11,dIdF12,dIdF21,dIdF22),dim=1)\n",
    "        else:\n",
    "            raise ValueError('Incorrect invariant index')\n",
    "    if secondDerivative:\n",
    "        dIdF = torch.zeros(F.shape[1],F.shape[1])\n",
    "        if(i==1):\n",
    "            # d(dI1/dF)/dF:\n",
    "            dIdF = 2.0*torch.eye(F.shape[1])\n",
    "        elif(i==3):\n",
    "            # d(dI3/dF)/dF:\n",
    "            J = F11*F22 - F12*F21\n",
    "            dJdF11 = F22\n",
    "            dJdF12 = - F21\n",
    "            dJdF21 = - F12\n",
    "            dJdF22 = F11\n",
    "            # d(dI3/dF)/dF11:\n",
    "            dIdF[0,0] = 2.0 * F22 * dJdF11\n",
    "            dIdF[0,1] = -2.0 * F21 * dJdF11\n",
    "            dIdF[0,2] = -2.0 * F12 * dJdF11\n",
    "            dIdF[0,3] = 2.0 * J + 2.0 * F11 * dJdF11\n",
    "            # d(dI3/dF)/dF12:\n",
    "            dIdF[1,0] = 2.0 * F22 * dJdF12\n",
    "            dIdF[1,1] = -2.0 * F21 * dJdF12\n",
    "            dIdF[1,2] = -2.0 * J -2.0 * F12 * dJdF12\n",
    "            dIdF[1,3] = 2.0 * F11 * dJdF12\n",
    "            # d(dI3/dF)/dF21:\n",
    "            dIdF[2,0] = 2.0 * F22 * dJdF21\n",
    "            dIdF[2,1] = -2.0 * J + -2.0 * F21 * dJdF21\n",
    "            dIdF[2,2] = -2.0 * F12 * dJdF21\n",
    "            dIdF[2,3] = 2.0 * F11 * dJdF21\n",
    "            # d(dI3/dF)/dF22:\n",
    "            dIdF[3,0] = 2.0 * J + 2.0 * F22 * dJdF22\n",
    "            dIdF[3,1] = -2.0 * F21 * dJdF22\n",
    "            dIdF[3,2] = -2.0 * F12 * dJdF22\n",
    "            dIdF[3,3] = 2.0 * F11 * dJdF22\n",
    "        else:\n",
    "            raise ValueError('Incorrect invariant index')\n",
    "    return dIdF    \n",
    "\n",
    "# def computeFeatures(I1, I2, I3):\n",
    "def computeFeatures(invariants):\n",
    "    \"\"\"\n",
    "    Compute the features dependent on the right Cauchy-Green strain invariants.\n",
    "    Note that the features only depend on I1 and I3.\n",
    "    \n",
    "    _Input Arguments_\n",
    "    \n",
    "    - `I1` - 1st invariant\n",
    "    \n",
    "    - `I2` - 2nd invariant\n",
    "\n",
    "    - `I3` - 3rd invariant\n",
    "    \n",
    "    _Output Arguments_\n",
    "    \n",
    "    - `x` - features\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # print('Call1')\n",
    "    I1, I2, I3 = invariants[:, 0], invariants[:, 1], invariants[:, 2] \n",
    "    #Generalized Mooney-Rivlin.\n",
    "    #The Gent-Thomas model cannot be represented by the generalized\n",
    "    #Mooney-Rivlin model. An additional feature has to be added.\n",
    "    considerGentThomas = True\n",
    "    #Polynomial terms degree.\n",
    "    Na = 7\n",
    "    #Volumetric terms degree.\n",
    "    Nb = 7\n",
    "\n",
    "    # print('Call2')\n",
    "    K1 = I1 * torch.pow(I3,-1/3) - 3.0\n",
    "    K2 = (I1 + I3 - 1) * torch.pow(I3,-2/3) - 3.0\n",
    "    J = torch.sqrt(I3)\n",
    "    #Calculate the number of features.\n",
    "    numFeatures = 0\n",
    "    #Polynomial terms (dependent on K1 and K2).\n",
    "    # print('Call3')\n",
    "    for n in range(Na):\n",
    "        numFeatures += n + 2\n",
    "    #Volumetric terms (dependent on J).\n",
    "    # print('Call4')\n",
    "    for m in range(Nb):\n",
    "        numFeatures += 1\n",
    "    #Additional Gent-Thomas feature.\n",
    "    # print('Call5')\n",
    "    if considerGentThomas:\n",
    "        numFeatures += 1\n",
    "    #Calculate the features.\n",
    "    x = torch.zeros(I1.shape[0],numFeatures)\n",
    "    i =- 1\n",
    "    \n",
    "    # print('Call6')\n",
    "    #Polynomial terms (dependent on K1 and K2).\n",
    "    for p in range(1,Na+1):\n",
    "        for q in range(p+1):\n",
    "            i+=1; x[:,i:(i+1)] = K1**(p-q) * K2**q\n",
    "\n",
    "    #Volumetric terms (dependent on J):\n",
    "    # print('Call7')\n",
    "    for m in range(1,Nb+1):\n",
    "        i+=1; x[:,i:(i+1)] = (J-1)**(2*m)\n",
    "        \n",
    "    #Additional Gent-Thomas feature.\n",
    "    # print('Call8')\n",
    "    if considerGentThomas:\n",
    "        i+=1; x[:,i:(i+1)] = torch.log((K2+3.0)/3.0)\n",
    "\n",
    "    # print('Call9')\n",
    "    \n",
    "    return x\n",
    "\n",
    "def getNumberOfFeatures():\n",
    "    \"\"\"\n",
    "    Compute number of features.\n",
    "    \n",
    "    _Input Arguments_\n",
    "    \n",
    "    - _none_\n",
    "    \n",
    "    _Output Arguments_\n",
    "    \n",
    "    - `features.shape[1]` - number of features\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    \"\"\"\n",
    "    features = computeFeatures(torch.zeros(1,3))\n",
    "    return features.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = np.linspace(10,60,6)\n",
    "\n",
    "for i in range(2,3):\n",
    "    \n",
    "    data = datasets[folders[i]][0]\n",
    "    \n",
    "    reaction = datasets[folders[i]][1]    \n",
    "    reaction = torch.tensor(reaction.values, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    Xe = data[(data['x_coor'] == 1.0)]\n",
    "    Xe = Xe.sort_values(by=['y_coor'])\n",
    "    Xw = data[(data['x_coor'] == 0.0)]\n",
    "    Xw = Xw.sort_values(by=['y_coor'])\n",
    "    Xn = data[(data['y_coor'] == 1.0)]\n",
    "    Xn = Xn.sort_values(by=['x_coor'])\n",
    "    Xs = data[(data['y_coor'] == 0.0)]\n",
    "    Xs = Xs.sort_values(by=['x_coor'])\n",
    "    X_e = Xe[['x_coor', 'y_coor']]\n",
    "    u_e = Xe[['u_x', 'u_y']]\n",
    "    X_w = Xw[['x_coor', 'y_coor']]\n",
    "    u_w = Xw[['u_x', 'u_y']]\n",
    "    X_n = Xn[['x_coor', 'y_coor']]\n",
    "    u_n = Xn[['u_x', 'u_y']]\n",
    "    X_s = Xs[['x_coor', 'y_coor']]\n",
    "    u_s = Xs[['u_x', 'u_y']]\n",
    "    \n",
    "    X_e = torch.tensor(X_e.values, dtype=torch.float32, requires_grad=True)\n",
    "    u_e = torch.tensor(u_e.values, dtype=torch.float32, requires_grad=True)\n",
    "    X_w = torch.tensor(X_w.values, dtype=torch.float32, requires_grad=True)\n",
    "    u_w = torch.tensor(u_w.values, dtype=torch.float32, requires_grad=True)\n",
    "    X_n = torch.tensor(X_n.values, dtype=torch.float32, requires_grad=True)\n",
    "    u_n = torch.tensor(u_n.values, dtype=torch.float32, requires_grad=True)\n",
    "    X_s = torch.tensor(X_s.values, dtype=torch.float32, requires_grad=True)\n",
    "    u_s = torch.tensor(u_s.values, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    \n",
    "    Xint = data[(data['x_coor'] != 0.0) & (data['x_coor'] != 1.0) & (data['y_coor'] != 0.0) & (data['y_coor'] != 1.0)]\n",
    "    \n",
    "    X_int = Xint[['x_coor', 'y_coor']]\n",
    "    u_int = Xint[['u_x', 'u_y']]\n",
    "    X_int = torch.tensor(X_int.values, dtype=torch.float32, requires_grad=True)\n",
    "    u_int = torch.tensor(u_int.values, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    batch_size = 62\n",
    "    num_train_samples = X_int.shape[0]//batch_size \n",
    "    \n",
    "    new_shape = (num_train_samples, batch_size, 2)\n",
    "\n",
    "    X_int = X_int.reshape(new_shape)\n",
    "    u_int = u_int.reshape(new_shape)\n",
    "    \n",
    "    X_bound = torch.stack((X_s, X_n, X_w, X_e), dim=0)\n",
    "    u_bound = torch.stack((u_s, u_n, u_w, u_e), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_bound.shape, u_bound.shape, X_int.shape, u_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Physics_Informed_NN(nn.Module):\n",
    "    \n",
    "    def __init__(self, layers, num_features, hyperparams=[0.1, 0.01, 0.001]):\n",
    "        \n",
    "        super(Physics_Informed_NN, self).__init__()\n",
    "        \n",
    "        self._activation = nn.Tanh()\n",
    "        self._layers = layers\n",
    "        self._num_layers = len(layers)\n",
    "        self._loss_function = nn.MSELoss(reduction ='mean')\n",
    "        self._hyperparams = hyperparams # [0.1, 0.1, 0.1]\n",
    "        self._num_features = num_features\n",
    "        \n",
    "        self._beta = nn.Parameter(torch.zeros((num_features, 1), requires_grad=True))\n",
    "        \n",
    "        self._linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        # print(self._linears[0].weight)\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self._linears[i].weight)\n",
    "            nn.init.ones_(self._linears[i].bias)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        for layer in self._linears[:-1]:\n",
    "            x = self._activation(layer(x))\n",
    "        x = self._linears[-1](x)\n",
    "        return x\n",
    "    \n",
    "    def loss_criterion(self, u_train, r_train, omega_train, loc=5):\n",
    "        \n",
    "        with torch.autograd.enable_grad():\n",
    "            # Deformation Tensor\n",
    "            u_train_hat = self.forward(omega_train)\n",
    "            grad_u_train = jacobian(self.forward, omega_train, create_graph=True)\n",
    "            \n",
    "            grad_u_train = torch.stack([grad_u_train[i, :, i, :] for i in range(grad_u_train.shape[0])])\n",
    "            I = torch.eye(2).unsqueeze(0).repeat(grad_u_train.shape[0], 1, 1)\n",
    "            F = I + grad_u_train\n",
    "            F = F.reshape(-1, 4)\n",
    "            \n",
    "            J = computeJacobian(F) # Calculate Jacobian\n",
    "            C = computeCauchyGreenStrain(F) # Calculate Cauchy-Green Strain\n",
    "            I1, I2, I3 = computeStrainInvariants(C) # Calculate Invariants\n",
    "            invariants = torch.stack([I1, I2, I3], dim=1)\n",
    "            invariants.requires_grad_(True)\n",
    "            \n",
    "            # Calculate Derivatives of Invariants\n",
    "            dIdF1 = computeStrainInvariantDerivatives(F, 1)\n",
    "            dIdF2 = computeStrainInvariantDerivatives(F, 2)\n",
    "            dIdF3 = computeStrainInvariantDerivatives(F, 3)\n",
    "            \n",
    "            Q = computeFeatures(invariants) # Obtain Features\n",
    "            # st = time.time()\n",
    "            grad_Q = jacobian(computeFeatures, invariants, create_graph=True) # Obtain Gradient of Features\n",
    "            # print(\"Time taken: \", time.time()-st)\n",
    "            grad_Q = torch.stack([grad_Q[i, :, i, :, 0] for i in range(grad_Q.shape[0])])\n",
    "            # print(grad_Q)\n",
    "            # Calculate Derivatives of Features\n",
    "            dQbdI1 = torch.matmul(grad_Q[:, :, 0], self._beta)\n",
    "            dQbdI2 = torch.matmul(grad_Q[:, :, 1], self._beta)\n",
    "            dQbdI3 = torch.matmul(grad_Q[:, :, 2], self._beta)\n",
    "            # Piola Kirchhoff Stress\n",
    "            P = dQbdI1 * dIdF1 + dQbdI2 * dIdF2 + dQbdI3 * dIdF3\n",
    "            \n",
    "            # grad_P = jacobian(self.eval_Piola, omega_train, create_graph=True)\n",
    "            \n",
    "            # u_train_hat, P = self.evaluate_params(omega_train=omega_train)\n",
    "            # grad_P = jacobian(self.evaluate_params, omega_train, create_graph=True)\n",
    "            \n",
    "            # print(P)\n",
    "            # Pxx, Pxy, Pyx, Pyy = P[:, 0], P[:, 1], P[:, 2], P[:, 3]\n",
    "            # omega_X, omega_Y = omega_train[:, 0], omega_train[:, 1]\n",
    "            # dPxxdx = torch.autograd.grad(Pxx, omega_X, grad_outputs=torch.ones(Pxx.shape[0]), create_graph=True, retain_graph=True, allow_unused=True)[0]\n",
    "            \n",
    "            # grad_P = jacobian(P, omega_train, create_graph=True)\n",
    "            \n",
    "            # print(\"omega_train: \", omega_train.shape)\n",
    "            # print(\"F: \", F.shape)\n",
    "            # print(\"J: \", J.shape)\n",
    "            # print(\"C: \", C.shape)\n",
    "            # print(\"I1: \", I1.shape)\n",
    "            # print(\"I2: \", I2.shape)\n",
    "            # print(\"I3: \", I3.shape)\n",
    "            # print(\"dIdF1: \", dIdF1.shape)\n",
    "            # print(\"dIdF2: \", dIdF2.shape)\n",
    "            # print(\"dIdF3: \", dIdF3.shape)\n",
    "            # print(\"Q: \", Q.shape)\n",
    "            # print(\"grad_Q: \", grad_Q.shape)\n",
    "            # print(\"dQbdI1: \", dQbdI1.shape)\n",
    "            # print(\"dQbdI2: \", dQbdI2.shape)\n",
    "            # print(\"dQbdI3: \", dQbdI3.shape)\n",
    "            # print(\"P: \", P.shape)\n",
    "            \n",
    "            \n",
    "            # print(\"grad_P: \", grad_P[0].shape)\n",
    "            # print(\"Pxx: \", Pxx.shape)\n",
    "            # print(\"Pxy: \", Pxy.shape)\n",
    "            # print(\"Pyx: \", Pyx.shape)\n",
    "            # print(\"Pyy: \", Pyy.shape)\n",
    "            # print(\"omega_X: \", omega_X.shape)\n",
    "            # print(\"omega_Y: \", omega_Y.shape)\n",
    "            # print(\"dPxxdx: \", dPxxdx.shape)\n",
    "            \n",
    "            # Regularization term\n",
    "            square_params, num = 0.0, 0\n",
    "            for param in self.parameters():\n",
    "                num += 1\n",
    "                square_params += torch.norm(param)**2  # L2 norm of parameters\n",
    "            # square_weights_sum = 0\n",
    "            # for layer in self._linears:\n",
    "            #     square_weights_sum += torch.square(layer.weight).sum()\n",
    "            # square_beta_sum = self._beta.sum()\n",
    "            # regularized_params = square_weights_sum + square_beta_sum\n",
    "                \n",
    "            # Experimental Loss on Interior Points\n",
    "            loss_exp = self._loss_function(u_train, u_train_hat)\n",
    "            \n",
    "            # Boundary Condition Loss\n",
    "            loss_bc = torch.tensor(0.0).reshape(1)\n",
    "            r_train_mod = torch.absolute(r_train)\n",
    "            # print(\"r_train_mod: \", r_train_mod)\n",
    "            if loc == 0: # South Boundary\n",
    "                x_coord = omega_train[:, 0]\n",
    "                piola_stress22 = P[:, 3]\n",
    "                piola_stress12 = P[:, 1]\n",
    "                resultant22 = (torch.trapz(piola_stress22, x_coord)).reshape(1)\n",
    "                resultant12 = (torch.trapz(piola_stress12, x_coord)).reshape(1)\n",
    "                loss_bc += (resultant22-r_train_mod[0])**2 + resultant12**2\n",
    "                # print(\"piola_stress: \", piola_stress.shape, x_coord.shape)\n",
    "                # print(\"resultant: \", resultant.shape)\n",
    "                # print(\"r_train_mod[0]: \", r_train_mod[0].shape)\n",
    "                # print(loss_bc.shape)\n",
    "            elif loc == 1: # North Boundary\n",
    "                x_coord = omega_train[:, 0]\n",
    "                piola_stress22 = P[:, 3]\n",
    "                piola_stress12 = P[:, 1]\n",
    "                resultant22 = (torch.trapz(piola_stress22, x_coord)).reshape(1)\n",
    "                resultant12 = (torch.trapz(piola_stress12, x_coord)).reshape(1)\n",
    "                loss_bc += (resultant22-r_train_mod[1])**2 + resultant12**2\n",
    "            elif loc == 2: # West Boundary\n",
    "                y_coord = omega_train[:, 1]\n",
    "                piola_stress11= P[:, 0]\n",
    "                piola_stress21 = P[:, 2]\n",
    "                resultant11 = (torch.trapz(piola_stress11, y_coord)).reshape(1)\n",
    "                resultant21 = (torch.trapz(piola_stress21, y_coord)).reshape(1)\n",
    "                loss_bc += (resultant11-r_train_mod[2])**2 + resultant21**2\n",
    "            elif loc == 3:\n",
    "                y_coord = omega_train[:, 1]\n",
    "                piola_stress11= P[:, 0]\n",
    "                piola_stress21 = P[:, 2]\n",
    "                resultant11 = (torch.trapz(piola_stress11, y_coord)).reshape(1)\n",
    "                resultant21 = (torch.trapz(piola_stress21, y_coord)).reshape(1)\n",
    "                loss_bc += (resultant11-r_train_mod[3])**2 + resultant21**2\n",
    "            \n",
    "            # PDE Loss\n",
    "            loss_pde = torch.tensor(0.0) # this loss was turning out to be very small, and the evaluation was very costly\n",
    "            \n",
    "            hp1, hp2, hp3 = self._hyperparams\n",
    "            # total_loss = loss_pde + hp1 * loss_exp + hp2 * loss_bc + hp3 * square_weights_sum\n",
    "            \n",
    "            total_loss = loss_pde + hp1 * loss_exp + hp2 * loss_bc + hp3 * square_params\n",
    "            \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============Epoch================> 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Loss: 0.3608272671699524\n",
      "Batch: 1, Loss: 0.3118214011192322\n",
      "Batch: 2, Loss: 0.336580365896225\n",
      "Batch: 3, Loss: 0.27754172682762146\n",
      "Batch: 4, Loss: 0.25083574652671814\n",
      "Batch: 5, Loss: 0.23620277643203735\n",
      "Batch: 6, Loss: 0.23149420320987701\n",
      "Batch: 7, Loss: 0.22779327630996704\n",
      "Batch: 8, Loss: 0.19587379693984985\n",
      "Batch: 9, Loss: 0.2053784728050232\n",
      "Batch: 10, Loss: 0.18808317184448242\n",
      "Batch: 11, Loss: 0.17987513542175293\n",
      "Batch: 12, Loss: 0.16422638297080994\n",
      "Batch: 13, Loss: 0.15662512183189392\n",
      "Batch: 14, Loss: 0.15521541237831116\n",
      "Batch: 15, Loss: 0.1431216299533844\n",
      "Batch: 16, Loss: 0.14266064763069153\n",
      "Batch: 17, Loss: 0.1355903595685959\n",
      "Batch: 18, Loss: 0.13119696080684662\n",
      "Batch: 19, Loss: 0.1208578571677208\n",
      "Batch: 20, Loss: 0.11886710673570633\n",
      "Batch: 21, Loss: 0.11172014474868774\n",
      "Batch: 22, Loss: 0.10797715187072754\n",
      "Batch: 23, Loss: 0.10393071174621582\n",
      "Batch: 24, Loss: 0.10484352707862854\n",
      "============Epoch================> 1\n",
      "Batch: 0, Loss: 0.14132048189640045\n",
      "Batch: 1, Loss: 0.12962505221366882\n",
      "Batch: 2, Loss: 0.13779795169830322\n",
      "Batch: 3, Loss: 0.12245391309261322\n",
      "Batch: 4, Loss: 0.09034371376037598\n",
      "Batch: 5, Loss: 0.08674511313438416\n",
      "Batch: 6, Loss: 0.08696833997964859\n",
      "Batch: 7, Loss: 0.08746382594108582\n",
      "Batch: 8, Loss: 0.08435138314962387\n",
      "Batch: 9, Loss: 0.08612103015184402\n",
      "Batch: 10, Loss: 0.0848081037402153\n",
      "Batch: 11, Loss: 0.08479323983192444\n",
      "Batch: 12, Loss: 0.08457780629396439\n",
      "Batch: 13, Loss: 0.08472137898206711\n",
      "Batch: 14, Loss: 0.08510059863328934\n",
      "Batch: 15, Loss: 0.08561123162508011\n",
      "Batch: 16, Loss: 0.08447089791297913\n",
      "Batch: 17, Loss: 0.08460178226232529\n",
      "Batch: 18, Loss: 0.08462221175432205\n",
      "Batch: 19, Loss: 0.08454423397779465\n",
      "Batch: 20, Loss: 0.08440348505973816\n",
      "Batch: 21, Loss: 0.08455415815114975\n",
      "Batch: 22, Loss: 0.08420522511005402\n",
      "Batch: 23, Loss: 0.0851675271987915\n",
      "Batch: 24, Loss: 0.08406039327383041\n",
      "============Epoch================> 2\n",
      "Batch: 0, Loss: 0.11956954747438431\n",
      "Batch: 1, Loss: 0.12356039881706238\n",
      "Batch: 2, Loss: 0.11865314096212387\n",
      "Batch: 3, Loss: 0.12199420481920242\n",
      "Batch: 4, Loss: 0.08449181914329529\n",
      "Batch: 5, Loss: 0.08309056609869003\n",
      "Batch: 6, Loss: 0.08225032687187195\n",
      "Batch: 7, Loss: 0.08203616738319397\n",
      "Batch: 8, Loss: 0.08343254774808884\n",
      "Batch: 9, Loss: 0.08178643137216568\n",
      "Batch: 10, Loss: 0.08209459483623505\n",
      "Batch: 11, Loss: 0.08202366530895233\n",
      "Batch: 12, Loss: 0.08208108693361282\n",
      "Batch: 13, Loss: 0.08199996501207352\n",
      "Batch: 14, Loss: 0.08186598867177963\n",
      "Batch: 15, Loss: 0.08210473507642746\n",
      "Batch: 16, Loss: 0.08133222162723541\n",
      "Batch: 17, Loss: 0.0812196359038353\n",
      "Batch: 18, Loss: 0.08141925930976868\n",
      "Batch: 19, Loss: 0.08071540296077728\n",
      "Batch: 20, Loss: 0.08099354058504105\n",
      "Batch: 21, Loss: 0.08069579303264618\n",
      "Batch: 22, Loss: 0.08042990416288376\n",
      "Batch: 23, Loss: 0.08109515905380249\n",
      "Batch: 24, Loss: 0.08166274428367615\n",
      "Training Time:  1297.6093933582306\n"
     ]
    }
   ],
   "source": [
    "layers = [2,50,10,2]\n",
    "model = Physics_Informed_NN(layers=layers, num_features=43)\n",
    "# model.loss_criterion(u_train=u_int[20], r_train=reaction, omega_train=X_int[20])\n",
    "\n",
    "BATCH = 25\n",
    "MAX_EPOCHS = 3\n",
    "\n",
    "# optimizer = optim.LBFGS(model.parameters(), lr=0.1, \n",
    "#                               max_iter=5,  \n",
    "#                               max_eval = None, \n",
    "#                               tolerance_grad = 1e-06, \n",
    "#                               tolerance_change = 1e-09, \n",
    "#                               history_size = 100, \n",
    "#                               line_search_fn = 'strong_wolfe')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001,betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    print(\"============Epoch================>\", epoch)\n",
    "    for batch in range(BATCH):\n",
    "        # print(f'Batch: {batch}')\n",
    "        if batch <= 3:\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                loss = model.loss_criterion(u_train=u_bound[batch], r_train=reaction, omega_train=X_bound[batch], loc=batch)\n",
    "                loss.backward()\n",
    "                print(f'Batch: {batch}, Loss: {loss.item()}') #  \n",
    "                return loss\n",
    "            optimizer.step(closure)\n",
    "        else:\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                loss = model.loss_criterion(u_train=u_int[batch-4], r_train=reaction, omega_train=X_int[batch-4])\n",
    "                print(f'Batch: {batch}, Loss: {loss.item()}')\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            optimizer.step(closure)\n",
    "            \n",
    "print('Training Time: ', time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The beta parameters of the model are, \n",
      "Index: 36, Beta: -0.03032270073890686\n",
      "Index: 2, Beta: -0.02136821858584881\n",
      "Index: 37, Beta: -0.02109895646572113\n",
      "Index: 43, Beta: -0.020868590101599693\n",
      "Index: 5, Beta: -0.019864879548549652\n",
      "Index: 1, Beta: -0.01938806287944317\n",
      "Index: 4, Beta: -0.019118856638669968\n",
      "Index: 3, Beta: -0.01821942627429962\n",
      "Index: 9, Beta: -0.014078034088015556\n",
      "Index: 8, Beta: -0.013385855592787266\n",
      "Index: 7, Beta: -0.012614504434168339\n",
      "Index: 6, Beta: -0.011757155880331993\n",
      "Index: 14, Beta: -0.006524367723613977\n",
      "Index: 13, Beta: -0.0055363597348332405\n",
      "Index: 12, Beta: -0.004533857572823763\n",
      "Index: 11, Beta: -0.0035458127968013287\n",
      "Index: 10, Beta: -0.002608978422358632\n",
      "Index: 38, Beta: -0.0017901933752000332\n",
      "Index: 39, Beta: -0.0004524534160736948\n",
      "Index: 25, Beta: -0.0004323547473177314\n",
      "Index: 26, Beta: -0.000312241812935099\n",
      "Index: 24, Beta: -0.00025708184693939984\n",
      "Index: 15, Beta: 0.00019133489695377648\n",
      "Index: 17, Beta: -0.0001383695926051587\n",
      "Index: 34, Beta: -0.00013587597641162574\n",
      "Index: 20, Beta: 0.00012012306251563132\n",
      "Index: 18, Beta: -9.284494444727898e-05\n",
      "Index: 33, Beta: -6.492012471426278e-05\n",
      "Index: 27, Beta: 6.166436651255935e-05\n",
      "Index: 28, Beta: 5.111666177981533e-05\n",
      "Index: 22, Beta: 4.772897591465153e-05\n",
      "Index: 16, Beta: -4.4926688133273274e-05\n",
      "Index: 29, Beta: 4.45780751761049e-05\n",
      "Index: 32, Beta: 4.195189103484154e-05\n",
      "Index: 19, Beta: 2.4489076167810708e-05\n",
      "Index: 23, Beta: -1.845398946898058e-05\n",
      "Index: 21, Beta: -1.8391579942544922e-05\n",
      "Index: 41, Beta: 1.3312072042026557e-05\n",
      "Index: 35, Beta: 1.239919220097363e-05\n",
      "Index: 31, Beta: 1.0179139280808158e-05\n",
      "Index: 42, Beta: 9.8017535492545e-06\n",
      "Index: 40, Beta: -4.8810870794113725e-06\n",
      "Index: 30, Beta: -4.519060894381255e-07\n"
     ]
    }
   ],
   "source": [
    "beta_params = model._beta\n",
    "\n",
    "# Get the indices for each value\n",
    "indices = torch.arange(beta_params.size(0))\n",
    "\n",
    "# Sort the absolute values of beta_params tensor and indices in descending order\n",
    "sorted_indices = torch.argsort(torch.abs(beta_params.squeeze()), descending=True)\n",
    "sorted_beta_params = beta_params[sorted_indices]\n",
    "\n",
    "# Print the corresponding indices and beta values\n",
    "print(\"The beta parameters of the model are, \")\n",
    "for i, beta in zip(sorted_indices, sorted_beta_params):\n",
    "    print(f\"Index: {i.item()+1}, Beta: {beta.item()}\")\n",
    "\n",
    "# for i, beta in enumerate(model._beta):\n",
    "#     print(f'{i+1} ====> {beta.item()}')\n",
    "# print(model._beta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
