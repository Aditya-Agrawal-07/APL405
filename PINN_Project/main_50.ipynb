{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd.functional import jacobian         # computation graph\n",
    "from torch import Tensor, nn, optim                 # tensor node in the computation graph\n",
    "# import torch.nn as nn                     # neural networks\n",
    "# import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "import time\n",
    "from scipy.integrate import simps\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the datasets from experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'exp_data'\n",
    "\n",
    "# List of all the folders in the base directory\n",
    "folders = [f for f in glob.glob(os.path.join(base_dir, '*')) if os.path.isdir(f)]\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for folder in folders:\n",
    "    # List of all the files in the folder\n",
    "    files = glob.glob(os.path.join(folder, '*.csv'))\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for file in files:\n",
    "        data.append(pd.read_csv(file))\n",
    "    \n",
    "    datasets[folder] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER Functions for course project\n",
    "\n",
    "def computeJacobian(F):\n",
    "    \"\"\"\n",
    "    Compute Jacobian from deformation gradient.\n",
    "    \n",
    "    _Input Arguments_\n",
    "    \n",
    "    - `F` - deformation gradient in Voigt notation\n",
    "    \n",
    "    _Output Arguments_\n",
    "    \n",
    "    - `J` - Jacobian\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    \"\"\"\n",
    "    F11 = F[:,0:1]\n",
    "    F12 = F[:,1:2]\n",
    "    F21 = F[:,2:3]\n",
    "    F22 = F[:,3:4]\n",
    "\n",
    "    J = F11*F22 - F12*F21\n",
    "    return J\n",
    "\n",
    "def computeCauchyGreenStrain(F):\n",
    "    \"\"\"\n",
    "    Compute right Cauchy-Green strain tensor from deformation gradient.\n",
    "    \n",
    "    _Input Arguments_\n",
    "    \n",
    "    - `F` - deformation gradient in Voigt notation\n",
    "    \n",
    "    _Output Arguments_\n",
    "    \n",
    "    - `C` - Cauchy-Green strain tensor in Voigt notation\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    \"\"\"\n",
    "    F11 = F[:,0:1]\n",
    "    F12 = F[:,1:2]\n",
    "    F21 = F[:,2:3]\n",
    "    F22 = F[:,3:4]\n",
    "\n",
    "    C11 = F11**2 + F21**2\n",
    "    C12 = F11*F12 + F21*F22\n",
    "    C21 = F11*F12 + F21*F22\n",
    "    C22 = F12**2 + F22**2\n",
    "\n",
    "    C = torch.cat((C11,C12,C21,C22),dim=1)\n",
    "    return C\n",
    "\n",
    "\n",
    "def computeStrainInvariants(C):\n",
    "    \"\"\"\n",
    "    Compute invariants of the Cauchy-Green strain tensor.\n",
    "    Plane strain is assumed.\n",
    "    \n",
    "    _Input Arguments_\n",
    "    \n",
    "    - `C` - Cauchy-Green strain tensor in Voigt notation\n",
    "    \n",
    "    _Output Arguments_\n",
    "    \n",
    "    - `I1` - 1st invariant\n",
    "    \n",
    "    - `I2` - 2nd invariant\n",
    "\n",
    "    - `I3` - 3rd invariant\n",
    "\n",
    "    ---\n",
    "    \n",
    "    \"\"\"\n",
    "    C11 = C[:,0:1]\n",
    "    C12 = C[:,1:2]\n",
    "    C21 = C[:,2:3]\n",
    "    C22 = C[:,3:4]\n",
    "\n",
    "    I1 = C11 + C22 + 1.0\n",
    "    I2 = C11 + C22 - C12*C21 + C11*C22\n",
    "    I3 = C11*C22 - C12*C21\n",
    "    return I1, I2, I3\n",
    "\n",
    "\n",
    "def computeStrainInvariantDerivatives(F,i,secondDerivative=False):\n",
    "    \"\"\"\n",
    "    Compute derivatives of the invariants of the Cauchy-Green strain tensor with respect to the deformation gradient.\n",
    "    Plane strain is assumed.\n",
    "    \n",
    "    _Input Arguments_\n",
    "    \n",
    "    - `F` - deformation gradient in Voigt notation\n",
    "\n",
    "    - `i` - specify the invariant that should be differentiated \n",
    "\n",
    "    - `secondDerivative` - specify if second derivative should be computed \n",
    "    \n",
    "    _Output Arguments_\n",
    "    \n",
    "    - `dIdF` - derivative (note that the size of `dIdF` depends on the choice of `secondDerivative`)\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    \"\"\"\n",
    "    F11 = F[:,0:1]\n",
    "    F12 = F[:,1:2]\n",
    "    F21 = F[:,2:3]\n",
    "    F22 = F[:,3:4]\n",
    "    if not(secondDerivative):\n",
    "        dIdF = torch.zeros(F.shape[0],F.shape[1])\n",
    "        if(i==1):\n",
    "            # dI1/dF:\n",
    "            dIdF = 2.0*F\n",
    "        elif(i==2):\n",
    "            # dI2/dF:\n",
    "            dIdF11 = 2.0*F11 - 2.0*F12*F21*F22 + 2.0*F11*(F22**2)\n",
    "            dIdF12 = 2.0*F12 + 2.0*F12*(F21**2) - 2.0*F11*F21*F22\n",
    "            dIdF21 = 2.0*F21 + 2.0*(F12**2)*F21 - 2.0*F11*F12*F22\n",
    "            dIdF22 = 2.0*F22 - 2.0*F11*F12*F21 + 2.0*(F11**2)*F22\n",
    "            dIdF = torch.cat((dIdF11,dIdF12,dIdF21,dIdF22),dim=1)\n",
    "        elif(i==3):\n",
    "            # dI3/dF:\n",
    "            J = F11*F22 - F12*F21\n",
    "            dIdF11 = 2.0*F22 * J\n",
    "            dIdF12 = -2.0*F21 * J\n",
    "            dIdF21 = -2.0*F12 * J\n",
    "            dIdF22 = 2.0*F11 * J\n",
    "            dIdF = torch.cat((dIdF11,dIdF12,dIdF21,dIdF22),dim=1)\n",
    "        else:\n",
    "            raise ValueError('Incorrect invariant index')\n",
    "    if secondDerivative:\n",
    "        dIdF = torch.zeros(F.shape[1],F.shape[1])\n",
    "        if(i==1):\n",
    "            # d(dI1/dF)/dF:\n",
    "            dIdF = 2.0*torch.eye(F.shape[1])\n",
    "        elif(i==3):\n",
    "            # d(dI3/dF)/dF:\n",
    "            J = F11*F22 - F12*F21\n",
    "            dJdF11 = F22\n",
    "            dJdF12 = - F21\n",
    "            dJdF21 = - F12\n",
    "            dJdF22 = F11\n",
    "            # d(dI3/dF)/dF11:\n",
    "            dIdF[0,0] = 2.0 * F22 * dJdF11\n",
    "            dIdF[0,1] = -2.0 * F21 * dJdF11\n",
    "            dIdF[0,2] = -2.0 * F12 * dJdF11\n",
    "            dIdF[0,3] = 2.0 * J + 2.0 * F11 * dJdF11\n",
    "            # d(dI3/dF)/dF12:\n",
    "            dIdF[1,0] = 2.0 * F22 * dJdF12\n",
    "            dIdF[1,1] = -2.0 * F21 * dJdF12\n",
    "            dIdF[1,2] = -2.0 * J -2.0 * F12 * dJdF12\n",
    "            dIdF[1,3] = 2.0 * F11 * dJdF12\n",
    "            # d(dI3/dF)/dF21:\n",
    "            dIdF[2,0] = 2.0 * F22 * dJdF21\n",
    "            dIdF[2,1] = -2.0 * J + -2.0 * F21 * dJdF21\n",
    "            dIdF[2,2] = -2.0 * F12 * dJdF21\n",
    "            dIdF[2,3] = 2.0 * F11 * dJdF21\n",
    "            # d(dI3/dF)/dF22:\n",
    "            dIdF[3,0] = 2.0 * J + 2.0 * F22 * dJdF22\n",
    "            dIdF[3,1] = -2.0 * F21 * dJdF22\n",
    "            dIdF[3,2] = -2.0 * F12 * dJdF22\n",
    "            dIdF[3,3] = 2.0 * F11 * dJdF22\n",
    "        else:\n",
    "            raise ValueError('Incorrect invariant index')\n",
    "    return dIdF    \n",
    "\n",
    "# def computeFeatures(I1, I2, I3):\n",
    "def computeFeatures(invariants):\n",
    "    \"\"\"\n",
    "    Compute the features dependent on the right Cauchy-Green strain invariants.\n",
    "    Note that the features only depend on I1 and I3.\n",
    "    \n",
    "    _Input Arguments_\n",
    "    \n",
    "    - `I1` - 1st invariant\n",
    "    \n",
    "    - `I2` - 2nd invariant\n",
    "\n",
    "    - `I3` - 3rd invariant\n",
    "    \n",
    "    _Output Arguments_\n",
    "    \n",
    "    - `x` - features\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # print('Call1')\n",
    "    I1, I2, I3 = invariants[:, 0], invariants[:, 1], invariants[:, 2] \n",
    "    #Generalized Mooney-Rivlin.\n",
    "    #The Gent-Thomas model cannot be represented by the generalized\n",
    "    #Mooney-Rivlin model. An additional feature has to be added.\n",
    "    considerGentThomas = True\n",
    "    #Polynomial terms degree.\n",
    "    Na = 7\n",
    "    #Volumetric terms degree.\n",
    "    Nb = 7\n",
    "\n",
    "    # print('Call2')\n",
    "    K1 = I1 * torch.pow(I3,-1/3) - 3.0\n",
    "    K2 = (I1 + I3 - 1) * torch.pow(I3,-2/3) - 3.0\n",
    "    J = torch.sqrt(I3)\n",
    "    #Calculate the number of features.\n",
    "    numFeatures = 0\n",
    "    #Polynomial terms (dependent on K1 and K2).\n",
    "    # print('Call3')\n",
    "    for n in range(Na):\n",
    "        numFeatures += n + 2\n",
    "    #Volumetric terms (dependent on J).\n",
    "    # print('Call4')\n",
    "    for m in range(Nb):\n",
    "        numFeatures += 1\n",
    "    #Additional Gent-Thomas feature.\n",
    "    # print('Call5')\n",
    "    if considerGentThomas:\n",
    "        numFeatures += 1\n",
    "    #Calculate the features.\n",
    "    x = torch.zeros(I1.shape[0],numFeatures)\n",
    "    i =- 1\n",
    "    \n",
    "    # print('Call6')\n",
    "    #Polynomial terms (dependent on K1 and K2).\n",
    "    for p in range(1,Na+1):\n",
    "        for q in range(p+1):\n",
    "            i+=1; x[:,i:(i+1)] = K1**(p-q) * K2**q\n",
    "\n",
    "    #Volumetric terms (dependent on J):\n",
    "    # print('Call7')\n",
    "    for m in range(1,Nb+1):\n",
    "        i+=1; x[:,i:(i+1)] = (J-1)**(2*m)\n",
    "        \n",
    "    #Additional Gent-Thomas feature.\n",
    "    # print('Call8')\n",
    "    if considerGentThomas:\n",
    "        i+=1; x[:,i:(i+1)] = torch.log((K2+3.0)/3.0)\n",
    "\n",
    "    # print('Call9')\n",
    "    \n",
    "    return x\n",
    "\n",
    "def getNumberOfFeatures():\n",
    "    \"\"\"\n",
    "    Compute number of features.\n",
    "    \n",
    "    _Input Arguments_\n",
    "    \n",
    "    - _none_\n",
    "    \n",
    "    _Output Arguments_\n",
    "    \n",
    "    - `features.shape[1]` - number of features\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    \"\"\"\n",
    "    features = computeFeatures(torch.zeros(1,3))\n",
    "    return features.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = np.linspace(10,60,6)\n",
    "\n",
    "for i in range(4,5):\n",
    "    \n",
    "    data = datasets[folders[i]][0]\n",
    "    \n",
    "    reaction = datasets[folders[i]][1]    \n",
    "    reaction = torch.tensor(reaction.values, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    Xe = data[(data['x_coor'] == 1.0)]\n",
    "    Xe = Xe.sort_values(by=['y_coor'])\n",
    "    Xw = data[(data['x_coor'] == 0.0)]\n",
    "    Xw = Xw.sort_values(by=['y_coor'])\n",
    "    Xn = data[(data['y_coor'] == 1.0)]\n",
    "    Xn = Xn.sort_values(by=['x_coor'])\n",
    "    Xs = data[(data['y_coor'] == 0.0)]\n",
    "    Xs = Xs.sort_values(by=['x_coor'])\n",
    "    X_e = Xe[['x_coor', 'y_coor']]\n",
    "    u_e = Xe[['u_x', 'u_y']]\n",
    "    X_w = Xw[['x_coor', 'y_coor']]\n",
    "    u_w = Xw[['u_x', 'u_y']]\n",
    "    X_n = Xn[['x_coor', 'y_coor']]\n",
    "    u_n = Xn[['u_x', 'u_y']]\n",
    "    X_s = Xs[['x_coor', 'y_coor']]\n",
    "    u_s = Xs[['u_x', 'u_y']]\n",
    "    \n",
    "    X_e = torch.tensor(X_e.values, dtype=torch.float32, requires_grad=True)\n",
    "    u_e = torch.tensor(u_e.values, dtype=torch.float32, requires_grad=True)\n",
    "    X_w = torch.tensor(X_w.values, dtype=torch.float32, requires_grad=True)\n",
    "    u_w = torch.tensor(u_w.values, dtype=torch.float32, requires_grad=True)\n",
    "    X_n = torch.tensor(X_n.values, dtype=torch.float32, requires_grad=True)\n",
    "    u_n = torch.tensor(u_n.values, dtype=torch.float32, requires_grad=True)\n",
    "    X_s = torch.tensor(X_s.values, dtype=torch.float32, requires_grad=True)\n",
    "    u_s = torch.tensor(u_s.values, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    \n",
    "    Xint = data[(data['x_coor'] != 0.0) & (data['x_coor'] != 1.0) & (data['y_coor'] != 0.0) & (data['y_coor'] != 1.0)]\n",
    "    \n",
    "    X_int = Xint[['x_coor', 'y_coor']]\n",
    "    u_int = Xint[['u_x', 'u_y']]\n",
    "    X_int = torch.tensor(X_int.values, dtype=torch.float32, requires_grad=True)\n",
    "    u_int = torch.tensor(u_int.values, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    batch_size = 62\n",
    "    num_train_samples = X_int.shape[0]//batch_size \n",
    "    \n",
    "    new_shape = (num_train_samples, batch_size, 2)\n",
    "\n",
    "    X_int = X_int.reshape(new_shape)\n",
    "    u_int = u_int.reshape(new_shape)\n",
    "    \n",
    "    X_bound = torch.stack((X_s, X_n, X_w, X_e), dim=0)\n",
    "    u_bound = torch.stack((u_s, u_n, u_w, u_e), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_bound.shape, u_bound.shape, X_int.shape, u_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Physics_Informed_NN(nn.Module):\n",
    "    \n",
    "    def __init__(self, layers, num_features, hyperparams=[0.1, 0.01, 0.001]):\n",
    "        \n",
    "        super(Physics_Informed_NN, self).__init__()\n",
    "        \n",
    "        self._activation = nn.Tanh()\n",
    "        self._layers = layers\n",
    "        self._num_layers = len(layers)\n",
    "        self._loss_function = nn.MSELoss(reduction ='mean')\n",
    "        self._hyperparams = hyperparams # [0.1, 0.1, 0.1]\n",
    "        self._num_features = num_features\n",
    "        \n",
    "        self._beta = nn.Parameter(torch.zeros((num_features, 1), requires_grad=True))\n",
    "        \n",
    "        self._linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        # print(self._linears[0].weight)\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self._linears[i].weight)\n",
    "            nn.init.ones_(self._linears[i].bias)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        for layer in self._linears[:-1]:\n",
    "            x = self._activation(layer(x))\n",
    "        x = self._linears[-1](x)\n",
    "        return x\n",
    "    \n",
    "    def loss_criterion(self, u_train, r_train, omega_train, loc=5):\n",
    "        \n",
    "        with torch.autograd.enable_grad():\n",
    "            # Deformation Tensor\n",
    "            u_train_hat = self.forward(omega_train)\n",
    "            grad_u_train = jacobian(self.forward, omega_train, create_graph=True)\n",
    "            \n",
    "            grad_u_train = torch.stack([grad_u_train[i, :, i, :] for i in range(grad_u_train.shape[0])])\n",
    "            I = torch.eye(2).unsqueeze(0).repeat(grad_u_train.shape[0], 1, 1)\n",
    "            F = I + grad_u_train\n",
    "            F = F.reshape(-1, 4)\n",
    "            \n",
    "            J = computeJacobian(F) # Calculate Jacobian\n",
    "            C = computeCauchyGreenStrain(F) # Calculate Cauchy-Green Strain\n",
    "            I1, I2, I3 = computeStrainInvariants(C) # Calculate Invariants\n",
    "            invariants = torch.stack([I1, I2, I3], dim=1)\n",
    "            invariants.requires_grad_(True)\n",
    "            \n",
    "            # Calculate Derivatives of Invariants\n",
    "            dIdF1 = computeStrainInvariantDerivatives(F, 1)\n",
    "            dIdF2 = computeStrainInvariantDerivatives(F, 2)\n",
    "            dIdF3 = computeStrainInvariantDerivatives(F, 3)\n",
    "            \n",
    "            Q = computeFeatures(invariants) # Obtain Features\n",
    "            # st = time.time()\n",
    "            grad_Q = jacobian(computeFeatures, invariants, create_graph=True) # Obtain Gradient of Features\n",
    "            # print(\"Time taken: \", time.time()-st)\n",
    "            grad_Q = torch.stack([grad_Q[i, :, i, :, 0] for i in range(grad_Q.shape[0])])\n",
    "            # print(grad_Q)\n",
    "            # Calculate Derivatives of Features\n",
    "            dQbdI1 = torch.matmul(grad_Q[:, :, 0], self._beta)\n",
    "            dQbdI2 = torch.matmul(grad_Q[:, :, 1], self._beta)\n",
    "            dQbdI3 = torch.matmul(grad_Q[:, :, 2], self._beta)\n",
    "            # Piola Kirchhoff Stress\n",
    "            P = dQbdI1 * dIdF1 + dQbdI2 * dIdF2 + dQbdI3 * dIdF3\n",
    "            \n",
    "            # grad_P = jacobian(self.eval_Piola, omega_train, create_graph=True)\n",
    "            \n",
    "            # u_train_hat, P = self.evaluate_params(omega_train=omega_train)\n",
    "            # grad_P = jacobian(self.evaluate_params, omega_train, create_graph=True)\n",
    "            \n",
    "            # print(P)\n",
    "            # Pxx, Pxy, Pyx, Pyy = P[:, 0], P[:, 1], P[:, 2], P[:, 3]\n",
    "            # omega_X, omega_Y = omega_train[:, 0], omega_train[:, 1]\n",
    "            # dPxxdx = torch.autograd.grad(Pxx, omega_X, grad_outputs=torch.ones(Pxx.shape[0]), create_graph=True, retain_graph=True, allow_unused=True)[0]\n",
    "            \n",
    "            # grad_P = jacobian(P, omega_train, create_graph=True)\n",
    "            \n",
    "            # print(\"omega_train: \", omega_train.shape)\n",
    "            # print(\"F: \", F.shape)\n",
    "            # print(\"J: \", J.shape)\n",
    "            # print(\"C: \", C.shape)\n",
    "            # print(\"I1: \", I1.shape)\n",
    "            # print(\"I2: \", I2.shape)\n",
    "            # print(\"I3: \", I3.shape)\n",
    "            # print(\"dIdF1: \", dIdF1.shape)\n",
    "            # print(\"dIdF2: \", dIdF2.shape)\n",
    "            # print(\"dIdF3: \", dIdF3.shape)\n",
    "            # print(\"Q: \", Q.shape)\n",
    "            # print(\"grad_Q: \", grad_Q.shape)\n",
    "            # print(\"dQbdI1: \", dQbdI1.shape)\n",
    "            # print(\"dQbdI2: \", dQbdI2.shape)\n",
    "            # print(\"dQbdI3: \", dQbdI3.shape)\n",
    "            # print(\"P: \", P.shape)\n",
    "            \n",
    "            \n",
    "            # print(\"grad_P: \", grad_P[0].shape)\n",
    "            # print(\"Pxx: \", Pxx.shape)\n",
    "            # print(\"Pxy: \", Pxy.shape)\n",
    "            # print(\"Pyx: \", Pyx.shape)\n",
    "            # print(\"Pyy: \", Pyy.shape)\n",
    "            # print(\"omega_X: \", omega_X.shape)\n",
    "            # print(\"omega_Y: \", omega_Y.shape)\n",
    "            # print(\"dPxxdx: \", dPxxdx.shape)\n",
    "            \n",
    "            # Regularization term\n",
    "            square_params, num = 0.0, 0\n",
    "            for param in self.parameters():\n",
    "                num += 1\n",
    "                square_params += torch.norm(param)**2  # L2 norm of parameters\n",
    "            # square_weights_sum = 0\n",
    "            # for layer in self._linears:\n",
    "            #     square_weights_sum += torch.square(layer.weight).sum()\n",
    "            # square_beta_sum = self._beta.sum()\n",
    "            # regularized_params = square_weights_sum + square_beta_sum\n",
    "                \n",
    "            # Experimental Loss on Interior Points\n",
    "            loss_exp = self._loss_function(u_train, u_train_hat)\n",
    "            \n",
    "            # Boundary Condition Loss\n",
    "            loss_bc = torch.tensor(0.0).reshape(1)\n",
    "            r_train_mod = torch.absolute(r_train)\n",
    "            # print(\"r_train_mod: \", r_train_mod)\n",
    "            if loc == 0: # South Boundary\n",
    "                x_coord = omega_train[:, 0]\n",
    "                piola_stress22 = P[:, 3]\n",
    "                piola_stress12 = P[:, 1]\n",
    "                resultant22 = (torch.trapz(piola_stress22, x_coord)).reshape(1)\n",
    "                resultant12 = (torch.trapz(piola_stress12, x_coord)).reshape(1)\n",
    "                loss_bc += (resultant22-r_train_mod[0])**2 + resultant12**2\n",
    "                # print(\"piola_stress: \", piola_stress.shape, x_coord.shape)\n",
    "                # print(\"resultant: \", resultant.shape)\n",
    "                # print(\"r_train_mod[0]: \", r_train_mod[0].shape)\n",
    "                # print(loss_bc.shape)\n",
    "            elif loc == 1: # North Boundary\n",
    "                x_coord = omega_train[:, 0]\n",
    "                piola_stress22 = P[:, 3]\n",
    "                piola_stress12 = P[:, 1]\n",
    "                resultant22 = (torch.trapz(piola_stress22, x_coord)).reshape(1)\n",
    "                resultant12 = (torch.trapz(piola_stress12, x_coord)).reshape(1)\n",
    "                loss_bc += (resultant22-r_train_mod[1])**2 + resultant12**2\n",
    "            elif loc == 2: # West Boundary\n",
    "                y_coord = omega_train[:, 1]\n",
    "                piola_stress11= P[:, 0]\n",
    "                piola_stress21 = P[:, 2]\n",
    "                resultant11 = (torch.trapz(piola_stress11, y_coord)).reshape(1)\n",
    "                resultant21 = (torch.trapz(piola_stress21, y_coord)).reshape(1)\n",
    "                loss_bc += (resultant11-r_train_mod[2])**2 + resultant21**2\n",
    "            elif loc == 3:\n",
    "                y_coord = omega_train[:, 1]\n",
    "                piola_stress11= P[:, 0]\n",
    "                piola_stress21 = P[:, 2]\n",
    "                resultant11 = (torch.trapz(piola_stress11, y_coord)).reshape(1)\n",
    "                resultant21 = (torch.trapz(piola_stress21, y_coord)).reshape(1)\n",
    "                loss_bc += (resultant11-r_train_mod[3])**2 + resultant21**2\n",
    "            \n",
    "            # PDE Loss\n",
    "            loss_pde = torch.tensor(0.0) # this loss was turning out to be very small, and the evaluation was very costly\n",
    "            \n",
    "            hp1, hp2, hp3 = self._hyperparams\n",
    "            # total_loss = loss_pde + hp1 * loss_exp + hp2 * loss_bc + hp3 * square_weights_sum\n",
    "            \n",
    "            total_loss = loss_pde + hp1 * loss_exp + hp2 * loss_bc + hp3 * square_params\n",
    "            \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============Epoch==========> 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Loss: 0.46947622299194336\n",
      "Batch: 1, Loss: 0.40085315704345703\n",
      "Batch: 2, Loss: 0.41580599546432495\n",
      "Batch: 3, Loss: 0.34524527192115784\n",
      "Batch: 4, Loss: 0.2343771904706955\n",
      "Batch: 5, Loss: 0.21819506585597992\n",
      "Batch: 6, Loss: 0.21684925258159637\n",
      "Batch: 7, Loss: 0.21571308374404907\n",
      "Batch: 8, Loss: 0.1790808141231537\n",
      "Batch: 9, Loss: 0.19580847024917603\n",
      "Batch: 10, Loss: 0.17636016011238098\n",
      "Batch: 11, Loss: 0.16910341382026672\n",
      "Batch: 12, Loss: 0.1523662656545639\n",
      "Batch: 13, Loss: 0.14581432938575745\n",
      "Batch: 14, Loss: 0.14682024717330933\n",
      "Batch: 15, Loss: 0.1346072256565094\n",
      "Batch: 16, Loss: 0.13504517078399658\n",
      "Batch: 17, Loss: 0.12931057810783386\n",
      "Batch: 18, Loss: 0.12505021691322327\n",
      "Batch: 19, Loss: 0.11402992904186249\n",
      "Batch: 20, Loss: 0.11336757987737656\n",
      "Batch: 21, Loss: 0.10633809864521027\n",
      "Batch: 22, Loss: 0.1030111238360405\n",
      "Batch: 23, Loss: 0.10017350316047668\n",
      "Batch: 24, Loss: 0.10264454036951065\n",
      "===============Epoch==========> 1\n",
      "Batch: 0, Loss: 0.2566910684108734\n",
      "Batch: 1, Loss: 0.2444545328617096\n",
      "Batch: 2, Loss: 0.2264782041311264\n",
      "Batch: 3, Loss: 0.21151721477508545\n",
      "Batch: 4, Loss: 0.09006215631961823\n",
      "Batch: 5, Loss: 0.08541052788496017\n",
      "Batch: 6, Loss: 0.08591049909591675\n",
      "Batch: 7, Loss: 0.08670637756586075\n",
      "Batch: 8, Loss: 0.08484460413455963\n",
      "Batch: 9, Loss: 0.08638542145490646\n",
      "Batch: 10, Loss: 0.08543238788843155\n",
      "Batch: 11, Loss: 0.08561402559280396\n",
      "Batch: 12, Loss: 0.08571846038103104\n",
      "Batch: 13, Loss: 0.08636286854743958\n",
      "Batch: 14, Loss: 0.08635833859443665\n",
      "Batch: 15, Loss: 0.087489053606987\n",
      "Batch: 16, Loss: 0.08531679213047028\n",
      "Batch: 17, Loss: 0.08556563407182693\n",
      "Batch: 18, Loss: 0.08570288866758347\n",
      "Batch: 19, Loss: 0.08532748371362686\n",
      "Batch: 20, Loss: 0.08522170037031174\n",
      "Batch: 21, Loss: 0.08545605838298798\n",
      "Batch: 22, Loss: 0.08491674810647964\n",
      "Batch: 23, Loss: 0.08669112622737885\n",
      "Batch: 24, Loss: 0.08539444208145142\n",
      "===============Epoch==========> 2\n",
      "Batch: 0, Loss: 0.23695941269397736\n",
      "Batch: 1, Loss: 0.2429947853088379\n",
      "Batch: 2, Loss: 0.209515780210495\n",
      "Batch: 3, Loss: 0.21302184462547302\n",
      "Batch: 4, Loss: 0.08621767908334732\n",
      "Batch: 5, Loss: 0.08333951979875565\n",
      "Batch: 6, Loss: 0.0823962464928627\n",
      "Batch: 7, Loss: 0.08243657648563385\n",
      "Batch: 8, Loss: 0.08397434651851654\n",
      "Batch: 9, Loss: 0.08241042494773865\n",
      "Batch: 10, Loss: 0.08270702511072159\n",
      "Batch: 11, Loss: 0.08274658024311066\n",
      "Batch: 12, Loss: 0.0825682207942009\n",
      "Batch: 13, Loss: 0.08292887359857559\n",
      "Batch: 14, Loss: 0.08282697200775146\n",
      "Batch: 15, Loss: 0.08333796262741089\n",
      "Batch: 16, Loss: 0.0820881649851799\n",
      "Batch: 17, Loss: 0.08219333738088608\n",
      "Batch: 18, Loss: 0.08248143643140793\n",
      "Batch: 19, Loss: 0.08116885274648666\n",
      "Batch: 20, Loss: 0.08178413659334183\n",
      "Batch: 21, Loss: 0.0813763290643692\n",
      "Batch: 22, Loss: 0.08096446096897125\n",
      "Batch: 23, Loss: 0.08232741057872772\n",
      "Batch: 24, Loss: 0.08351408690214157\n",
      "Training Time:  1725.0156118869781\n"
     ]
    }
   ],
   "source": [
    "layers = [2,50,10,2]\n",
    "model = Physics_Informed_NN(layers=layers, num_features=43)\n",
    "# model.loss_criterion(u_train=u_int[20], r_train=reaction, omega_train=X_int[20])\n",
    "\n",
    "BATCH = 25\n",
    "MAX_EPOCHS = 3\n",
    "\n",
    "# optimizer = optim.LBFGS(model.parameters(), lr=0.1, \n",
    "#                               max_iter=5,  \n",
    "#                               max_eval = None, \n",
    "#                               tolerance_grad = 1e-06, \n",
    "#                               tolerance_change = 1e-09, \n",
    "#                               history_size = 100, \n",
    "#                               line_search_fn = 'strong_wolfe')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    print(\"===============Epoch==========>\", epoch)\n",
    "    # interior_index, boundary_index = 0, 0\n",
    "    for batch in range(BATCH):\n",
    "        # print(f'Batch: {batch}')\n",
    "        if batch <= 3:\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                loss = model.loss_criterion(u_train=u_bound[batch], r_train=reaction, omega_train=X_bound[batch], loc=batch)\n",
    "                # boundary_index += 1\n",
    "                loss.backward()\n",
    "                print(f'Batch: {batch}, Loss: {loss.item()}') # Batch: {batch}, \n",
    "                return loss\n",
    "            optimizer.step(closure)\n",
    "        else:\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                loss = model.loss_criterion(u_train=u_int[batch - 4], r_train=reaction, omega_train=X_int[batch - 4])\n",
    "                # interior_index += 1\n",
    "                print(f'Batch: {batch}, Loss: {loss.item()}')\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            optimizer.step(closure)\n",
    "            \n",
    "print('Training Time: ', time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The beta parameters of the model are, \n",
      "Index: 36, Beta: -0.030564846470952034\n",
      "Index: 37, Beta: -0.02375234104692936\n",
      "Index: 2, Beta: -0.021825416013598442\n",
      "Index: 43, Beta: -0.021552903577685356\n",
      "Index: 5, Beta: -0.020494718104600906\n",
      "Index: 4, Beta: -0.019802140071988106\n",
      "Index: 1, Beta: -0.01978423446416855\n",
      "Index: 3, Beta: -0.018952779471874237\n",
      "Index: 9, Beta: -0.016391858458518982\n",
      "Index: 8, Beta: -0.01585765741765499\n",
      "Index: 7, Beta: -0.015241487883031368\n",
      "Index: 6, Beta: -0.014529050327837467\n",
      "Index: 14, Beta: -0.009953312575817108\n",
      "Index: 13, Beta: -0.00916887167841196\n",
      "Index: 12, Beta: -0.008285663090646267\n",
      "Index: 11, Beta: -0.0073189339600503445\n",
      "Index: 10, Beta: -0.006290603894740343\n",
      "Index: 38, Beta: -0.005618561524897814\n",
      "Index: 20, Beta: -0.0010095395846292377\n",
      "Index: 25, Beta: 0.00063336017774418\n",
      "Index: 39, Beta: 0.0006062289467081428\n",
      "Index: 24, Beta: 0.0005178358405828476\n",
      "Index: 26, Beta: 0.0005088273901492357\n",
      "Index: 19, Beta: -0.00045949884224683046\n",
      "Index: 21, Beta: -0.00040295315557159483\n",
      "Index: 27, Beta: 0.00024817048688419163\n",
      "Index: 22, Beta: -0.0002064005529973656\n",
      "Index: 23, Beta: 0.00018045429897028953\n",
      "Index: 31, Beta: 0.00016642859554849565\n",
      "Index: 32, Beta: 0.0001175951911136508\n",
      "Index: 16, Beta: 0.00011401007213862613\n",
      "Index: 29, Beta: -0.00010280995775246993\n",
      "Index: 17, Beta: 9.938198490999639e-05\n",
      "Index: 28, Beta: -9.068212239071727e-05\n",
      "Index: 18, Beta: -8.566455653635785e-05\n",
      "Index: 40, Beta: -5.859804150532e-05\n",
      "Index: 34, Beta: -5.2349925681483e-05\n",
      "Index: 30, Beta: 5.212281394051388e-05\n",
      "Index: 35, Beta: 2.8446034775697626e-05\n",
      "Index: 15, Beta: 1.3964428944746032e-05\n",
      "Index: 41, Beta: -1.0297783774149138e-05\n",
      "Index: 33, Beta: -6.92333560436964e-06\n",
      "Index: 42, Beta: 2.956993739644531e-06\n"
     ]
    }
   ],
   "source": [
    "beta_params = model._beta\n",
    "\n",
    "# Get the indices for each value\n",
    "indices = torch.arange(beta_params.size(0))\n",
    "\n",
    "# Sort the absolute values of beta_params tensor and indices in descending order\n",
    "sorted_indices = torch.argsort(torch.abs(beta_params.squeeze()), descending=True)\n",
    "sorted_beta_params = beta_params[sorted_indices]\n",
    "\n",
    "# Print the corresponding indices and beta values\n",
    "print(\"The beta parameters of the model are, \")\n",
    "for i, beta in zip(sorted_indices, sorted_beta_params):\n",
    "    print(f\"Index: {i.item()+1}, Beta: {beta.item()}\")\n",
    "\n",
    "# for i, beta in enumerate(model._beta):\n",
    "#     print(f'{i+1} ====> {beta.item()}')\n",
    "# print(model._beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
