{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd.functional import jacobian         # computation graph\n",
    "from torch import Tensor, nn, optim                 # tensor node in the computation graph\n",
    "# import torch.nn as nn                     # neural networks\n",
    "# import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "import time\n",
    "from scipy.integrate import simps\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the datasets from experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'exp_data'\n",
    "\n",
    "# List of all the folders in the base directory\n",
    "folders = [f for f in glob.glob(os.path.join(base_dir, '*')) if os.path.isdir(f)]\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for folder in folders:\n",
    "    # List of all the files in the folder\n",
    "    files = glob.glob(os.path.join(folder, '*.csv'))\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for file in files:\n",
    "        data.append(pd.read_csv(file))\n",
    "    \n",
    "    datasets[folder] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER Functions for course project\n",
    "\n",
    "def computeJacobian(F):\n",
    "    \"\"\"\n",
    "    Compute Jacobian from deformation gradient.\n",
    "    \n",
    "    _Input Arguments_\n",
    "    \n",
    "    - `F` - deformation gradient in Voigt notation\n",
    "    \n",
    "    _Output Arguments_\n",
    "    \n",
    "    - `J` - Jacobian\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    \"\"\"\n",
    "    F11 = F[:,0:1]\n",
    "    F12 = F[:,1:2]\n",
    "    F21 = F[:,2:3]\n",
    "    F22 = F[:,3:4]\n",
    "\n",
    "    J = F11*F22 - F12*F21\n",
    "    return J\n",
    "\n",
    "def computeCauchyGreenStrain(F):\n",
    "    \"\"\"\n",
    "    Compute right Cauchy-Green strain tensor from deformation gradient.\n",
    "    \n",
    "    _Input Arguments_\n",
    "    \n",
    "    - `F` - deformation gradient in Voigt notation\n",
    "    \n",
    "    _Output Arguments_\n",
    "    \n",
    "    - `C` - Cauchy-Green strain tensor in Voigt notation\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    \"\"\"\n",
    "    F11 = F[:,0:1]\n",
    "    F12 = F[:,1:2]\n",
    "    F21 = F[:,2:3]\n",
    "    F22 = F[:,3:4]\n",
    "\n",
    "    C11 = F11**2 + F21**2\n",
    "    C12 = F11*F12 + F21*F22\n",
    "    C21 = F11*F12 + F21*F22\n",
    "    C22 = F12**2 + F22**2\n",
    "\n",
    "    C = torch.cat((C11,C12,C21,C22),dim=1)\n",
    "    return C\n",
    "\n",
    "\n",
    "def computeStrainInvariants(C):\n",
    "    \"\"\"\n",
    "    Compute invariants of the Cauchy-Green strain tensor.\n",
    "    Plane strain is assumed.\n",
    "    \n",
    "    _Input Arguments_\n",
    "    \n",
    "    - `C` - Cauchy-Green strain tensor in Voigt notation\n",
    "    \n",
    "    _Output Arguments_\n",
    "    \n",
    "    - `I1` - 1st invariant\n",
    "    \n",
    "    - `I2` - 2nd invariant\n",
    "\n",
    "    - `I3` - 3rd invariant\n",
    "\n",
    "    ---\n",
    "    \n",
    "    \"\"\"\n",
    "    C11 = C[:,0:1]\n",
    "    C12 = C[:,1:2]\n",
    "    C21 = C[:,2:3]\n",
    "    C22 = C[:,3:4]\n",
    "\n",
    "    I1 = C11 + C22 + 1.0\n",
    "    I2 = C11 + C22 - C12*C21 + C11*C22\n",
    "    I3 = C11*C22 - C12*C21\n",
    "    return I1, I2, I3\n",
    "\n",
    "\n",
    "def computeStrainInvariantDerivatives(F,i,secondDerivative=False):\n",
    "    \"\"\"\n",
    "    Compute derivatives of the invariants of the Cauchy-Green strain tensor with respect to the deformation gradient.\n",
    "    Plane strain is assumed.\n",
    "    \n",
    "    _Input Arguments_\n",
    "    \n",
    "    - `F` - deformation gradient in Voigt notation\n",
    "\n",
    "    - `i` - specify the invariant that should be differentiated \n",
    "\n",
    "    - `secondDerivative` - specify if second derivative should be computed \n",
    "    \n",
    "    _Output Arguments_\n",
    "    \n",
    "    - `dIdF` - derivative (note that the size of `dIdF` depends on the choice of `secondDerivative`)\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    \"\"\"\n",
    "    F11 = F[:,0:1]\n",
    "    F12 = F[:,1:2]\n",
    "    F21 = F[:,2:3]\n",
    "    F22 = F[:,3:4]\n",
    "    if not(secondDerivative):\n",
    "        dIdF = torch.zeros(F.shape[0],F.shape[1])\n",
    "        if(i==1):\n",
    "            # dI1/dF:\n",
    "            dIdF = 2.0*F\n",
    "        elif(i==2):\n",
    "            # dI2/dF:\n",
    "            dIdF11 = 2.0*F11 - 2.0*F12*F21*F22 + 2.0*F11*(F22**2)\n",
    "            dIdF12 = 2.0*F12 + 2.0*F12*(F21**2) - 2.0*F11*F21*F22\n",
    "            dIdF21 = 2.0*F21 + 2.0*(F12**2)*F21 - 2.0*F11*F12*F22\n",
    "            dIdF22 = 2.0*F22 - 2.0*F11*F12*F21 + 2.0*(F11**2)*F22\n",
    "            dIdF = torch.cat((dIdF11,dIdF12,dIdF21,dIdF22),dim=1)\n",
    "        elif(i==3):\n",
    "            # dI3/dF:\n",
    "            J = F11*F22 - F12*F21\n",
    "            dIdF11 = 2.0*F22 * J\n",
    "            dIdF12 = -2.0*F21 * J\n",
    "            dIdF21 = -2.0*F12 * J\n",
    "            dIdF22 = 2.0*F11 * J\n",
    "            dIdF = torch.cat((dIdF11,dIdF12,dIdF21,dIdF22),dim=1)\n",
    "        else:\n",
    "            raise ValueError('Incorrect invariant index')\n",
    "    if secondDerivative:\n",
    "        dIdF = torch.zeros(F.shape[1],F.shape[1])\n",
    "        if(i==1):\n",
    "            # d(dI1/dF)/dF:\n",
    "            dIdF = 2.0*torch.eye(F.shape[1])\n",
    "        elif(i==3):\n",
    "            # d(dI3/dF)/dF:\n",
    "            J = F11*F22 - F12*F21\n",
    "            dJdF11 = F22\n",
    "            dJdF12 = - F21\n",
    "            dJdF21 = - F12\n",
    "            dJdF22 = F11\n",
    "            # d(dI3/dF)/dF11:\n",
    "            dIdF[0,0] = 2.0 * F22 * dJdF11\n",
    "            dIdF[0,1] = -2.0 * F21 * dJdF11\n",
    "            dIdF[0,2] = -2.0 * F12 * dJdF11\n",
    "            dIdF[0,3] = 2.0 * J + 2.0 * F11 * dJdF11\n",
    "            # d(dI3/dF)/dF12:\n",
    "            dIdF[1,0] = 2.0 * F22 * dJdF12\n",
    "            dIdF[1,1] = -2.0 * F21 * dJdF12\n",
    "            dIdF[1,2] = -2.0 * J -2.0 * F12 * dJdF12\n",
    "            dIdF[1,3] = 2.0 * F11 * dJdF12\n",
    "            # d(dI3/dF)/dF21:\n",
    "            dIdF[2,0] = 2.0 * F22 * dJdF21\n",
    "            dIdF[2,1] = -2.0 * J + -2.0 * F21 * dJdF21\n",
    "            dIdF[2,2] = -2.0 * F12 * dJdF21\n",
    "            dIdF[2,3] = 2.0 * F11 * dJdF21\n",
    "            # d(dI3/dF)/dF22:\n",
    "            dIdF[3,0] = 2.0 * J + 2.0 * F22 * dJdF22\n",
    "            dIdF[3,1] = -2.0 * F21 * dJdF22\n",
    "            dIdF[3,2] = -2.0 * F12 * dJdF22\n",
    "            dIdF[3,3] = 2.0 * F11 * dJdF22\n",
    "        else:\n",
    "            raise ValueError('Incorrect invariant index')\n",
    "    return dIdF    \n",
    "\n",
    "# def computeFeatures(I1, I2, I3):\n",
    "def computeFeatures(invariants):\n",
    "    \"\"\"\n",
    "    Compute the features dependent on the right Cauchy-Green strain invariants.\n",
    "    Note that the features only depend on I1 and I3.\n",
    "    \n",
    "    _Input Arguments_\n",
    "    \n",
    "    - `I1` - 1st invariant\n",
    "    \n",
    "    - `I2` - 2nd invariant\n",
    "\n",
    "    - `I3` - 3rd invariant\n",
    "    \n",
    "    _Output Arguments_\n",
    "    \n",
    "    - `x` - features\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # print('Call1')\n",
    "    I1, I2, I3 = invariants[:, 0], invariants[:, 1], invariants[:, 2] \n",
    "    #Generalized Mooney-Rivlin.\n",
    "    #The Gent-Thomas model cannot be represented by the generalized\n",
    "    #Mooney-Rivlin model. An additional feature has to be added.\n",
    "    considerGentThomas = True\n",
    "    #Polynomial terms degree.\n",
    "    Na = 7\n",
    "    #Volumetric terms degree.\n",
    "    Nb = 7\n",
    "\n",
    "    # print('Call2')\n",
    "    K1 = I1 * torch.pow(I3,-1/3) - 3.0\n",
    "    K2 = (I1 + I3 - 1) * torch.pow(I3,-2/3) - 3.0\n",
    "    J = torch.sqrt(I3)\n",
    "    #Calculate the number of features.\n",
    "    numFeatures = 0\n",
    "    #Polynomial terms (dependent on K1 and K2).\n",
    "    # print('Call3')\n",
    "    for n in range(Na):\n",
    "        numFeatures += n + 2\n",
    "    #Volumetric terms (dependent on J).\n",
    "    # print('Call4')\n",
    "    for m in range(Nb):\n",
    "        numFeatures += 1\n",
    "    #Additional Gent-Thomas feature.\n",
    "    # print('Call5')\n",
    "    if considerGentThomas:\n",
    "        numFeatures += 1\n",
    "    #Calculate the features.\n",
    "    x = torch.zeros(I1.shape[0],numFeatures)\n",
    "    i =- 1\n",
    "    \n",
    "    # print('Call6')\n",
    "    #Polynomial terms (dependent on K1 and K2).\n",
    "    for p in range(1,Na+1):\n",
    "        for q in range(p+1):\n",
    "            i+=1; x[:,i:(i+1)] = K1**(p-q) * K2**q\n",
    "\n",
    "    #Volumetric terms (dependent on J):\n",
    "    # print('Call7')\n",
    "    for m in range(1,Nb+1):\n",
    "        i+=1; x[:,i:(i+1)] = (J-1)**(2*m)\n",
    "        \n",
    "    #Additional Gent-Thomas feature.\n",
    "    # print('Call8')\n",
    "    if considerGentThomas:\n",
    "        i+=1; x[:,i:(i+1)] = torch.log((K2+3.0)/3.0)\n",
    "\n",
    "    # print('Call9')\n",
    "    \n",
    "    return x\n",
    "\n",
    "def getNumberOfFeatures():\n",
    "    \"\"\"\n",
    "    Compute number of features.\n",
    "    \n",
    "    _Input Arguments_\n",
    "    \n",
    "    - _none_\n",
    "    \n",
    "    _Output Arguments_\n",
    "    \n",
    "    - `features.shape[1]` - number of features\n",
    "    \n",
    "    ---\n",
    "    \n",
    "    \"\"\"\n",
    "    features = computeFeatures(torch.zeros(1,3))\n",
    "    return features.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = np.linspace(10,60,6)\n",
    "\n",
    "for i in range(0,1):\n",
    "    \n",
    "    data = datasets[folders[i]][0]\n",
    "    \n",
    "    reaction = datasets[folders[i]][1]    \n",
    "    reaction = torch.tensor(reaction.values, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    Xe = data[(data['x_coor'] == 1.0)]\n",
    "    Xe = Xe.sort_values(by=['y_coor'])\n",
    "    Xw = data[(data['x_coor'] == 0.0)]\n",
    "    Xw = Xw.sort_values(by=['y_coor'])\n",
    "    Xn = data[(data['y_coor'] == 1.0)]\n",
    "    Xn = Xn.sort_values(by=['x_coor'])\n",
    "    Xs = data[(data['y_coor'] == 0.0)]\n",
    "    Xs = Xs.sort_values(by=['x_coor'])\n",
    "    X_e = Xe[['x_coor', 'y_coor']]\n",
    "    u_e = Xe[['u_x', 'u_y']]\n",
    "    X_w = Xw[['x_coor', 'y_coor']]\n",
    "    u_w = Xw[['u_x', 'u_y']]\n",
    "    X_n = Xn[['x_coor', 'y_coor']]\n",
    "    u_n = Xn[['u_x', 'u_y']]\n",
    "    X_s = Xs[['x_coor', 'y_coor']]\n",
    "    u_s = Xs[['u_x', 'u_y']]\n",
    "    \n",
    "    X_e = torch.tensor(X_e.values, dtype=torch.float32, requires_grad=True)\n",
    "    u_e = torch.tensor(u_e.values, dtype=torch.float32, requires_grad=True)\n",
    "    X_w = torch.tensor(X_w.values, dtype=torch.float32, requires_grad=True)\n",
    "    u_w = torch.tensor(u_w.values, dtype=torch.float32, requires_grad=True)\n",
    "    X_n = torch.tensor(X_n.values, dtype=torch.float32, requires_grad=True)\n",
    "    u_n = torch.tensor(u_n.values, dtype=torch.float32, requires_grad=True)\n",
    "    X_s = torch.tensor(X_s.values, dtype=torch.float32, requires_grad=True)\n",
    "    u_s = torch.tensor(u_s.values, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    \n",
    "    Xint = data[(data['x_coor'] != 0.0) & (data['x_coor'] != 1.0) & (data['y_coor'] != 0.0) & (data['y_coor'] != 1.0)]\n",
    "    \n",
    "    X_int = Xint[['x_coor', 'y_coor']]\n",
    "    u_int = Xint[['u_x', 'u_y']]\n",
    "    X_int = torch.tensor(X_int.values, dtype=torch.float32, requires_grad=True)\n",
    "    u_int = torch.tensor(u_int.values, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    batch_size = 62\n",
    "    num_train_samples = X_int.shape[0]//batch_size \n",
    "    \n",
    "    new_shape = (num_train_samples, batch_size, 2)\n",
    "\n",
    "    X_int = X_int.reshape(new_shape)\n",
    "    u_int = u_int.reshape(new_shape)\n",
    "    \n",
    "    X_bound = torch.stack((X_s, X_n, X_w, X_e), dim=0)\n",
    "    u_bound = torch.stack((u_s, u_n, u_w, u_e), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_bound.shape, u_bound.shape, X_int.shape, u_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Physics_Informed_NN(nn.Module):\n",
    "    \n",
    "    def __init__(self, layers, num_features, hyperparams=[0.1, 0.01, 0.001]):\n",
    "        \n",
    "        super(Physics_Informed_NN, self).__init__()\n",
    "        \n",
    "        self._activation = nn.Tanh()\n",
    "        self._layers = layers\n",
    "        self._num_layers = len(layers)\n",
    "        self._loss_function = nn.MSELoss(reduction ='mean')\n",
    "        self._hyperparams = hyperparams # [0.1, 0.1, 0.1]\n",
    "        self._num_features = num_features\n",
    "        \n",
    "        self._beta = nn.Parameter(torch.zeros((num_features, 1), requires_grad=True))\n",
    "        \n",
    "        self._linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        # print(self._linears[0].weight)\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self._linears[i].weight)\n",
    "            nn.init.ones_(self._linears[i].bias)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        for layer in self._linears[:-1]:\n",
    "            x = self._activation(layer(x))\n",
    "        x = self._linears[-1](x)\n",
    "        return x\n",
    "    \n",
    "    def loss_criterion(self, u_train, r_train, omega_train, loc=5):\n",
    "        \n",
    "        with torch.autograd.enable_grad():\n",
    "            # Deformation Tensor\n",
    "            u_train_hat = self.forward(omega_train)\n",
    "            grad_u_train = jacobian(self.forward, omega_train, create_graph=True)\n",
    "            \n",
    "            grad_u_train = torch.stack([grad_u_train[i, :, i, :] for i in range(grad_u_train.shape[0])])\n",
    "            I = torch.eye(2).unsqueeze(0).repeat(grad_u_train.shape[0], 1, 1)\n",
    "            F = I + grad_u_train\n",
    "            F = F.reshape(-1, 4)\n",
    "            \n",
    "            J = computeJacobian(F) # Calculate Jacobian\n",
    "            C = computeCauchyGreenStrain(F) # Calculate Cauchy-Green Strain\n",
    "            I1, I2, I3 = computeStrainInvariants(C) # Calculate Invariants\n",
    "            invariants = torch.stack([I1, I2, I3], dim=1)\n",
    "            invariants.requires_grad_(True)\n",
    "            \n",
    "            # Calculate Derivatives of Invariants\n",
    "            dIdF1 = computeStrainInvariantDerivatives(F, 1)\n",
    "            dIdF2 = computeStrainInvariantDerivatives(F, 2)\n",
    "            dIdF3 = computeStrainInvariantDerivatives(F, 3)\n",
    "            \n",
    "            Q = computeFeatures(invariants) # Obtain Features\n",
    "            # st = time.time()\n",
    "            grad_Q = jacobian(computeFeatures, invariants, create_graph=True) # Obtain Gradient of Features\n",
    "            # print(\"Time taken: \", time.time()-st)\n",
    "            grad_Q = torch.stack([grad_Q[i, :, i, :, 0] for i in range(grad_Q.shape[0])])\n",
    "            # print(grad_Q)\n",
    "            # Calculate Derivatives of Features\n",
    "            dQbdI1 = torch.matmul(grad_Q[:, :, 0], self._beta)\n",
    "            dQbdI2 = torch.matmul(grad_Q[:, :, 1], self._beta)\n",
    "            dQbdI3 = torch.matmul(grad_Q[:, :, 2], self._beta)\n",
    "            # Piola Kirchhoff Stress\n",
    "            P = dQbdI1 * dIdF1 + dQbdI2 * dIdF2 + dQbdI3 * dIdF3\n",
    "            \n",
    "            # grad_P = jacobian(self.eval_Piola, omega_train, create_graph=True)\n",
    "            \n",
    "            # u_train_hat, P = self.evaluate_params(omega_train=omega_train)\n",
    "            # grad_P = jacobian(self.evaluate_params, omega_train, create_graph=True)\n",
    "            \n",
    "            # print(P)\n",
    "            # Pxx, Pxy, Pyx, Pyy = P[:, 0], P[:, 1], P[:, 2], P[:, 3]\n",
    "            # omega_X, omega_Y = omega_train[:, 0], omega_train[:, 1]\n",
    "            # dPxxdx = torch.autograd.grad(Pxx, omega_X, grad_outputs=torch.ones(Pxx.shape[0]), create_graph=True, retain_graph=True, allow_unused=True)[0]\n",
    "            \n",
    "            # grad_P = jacobian(P, omega_train, create_graph=True)\n",
    "            \n",
    "            # print(\"omega_train: \", omega_train.shape)\n",
    "            # print(\"F: \", F.shape)\n",
    "            # print(\"J: \", J.shape)\n",
    "            # print(\"C: \", C.shape)\n",
    "            # print(\"I1: \", I1.shape)\n",
    "            # print(\"I2: \", I2.shape)\n",
    "            # print(\"I3: \", I3.shape)\n",
    "            # print(\"dIdF1: \", dIdF1.shape)\n",
    "            # print(\"dIdF2: \", dIdF2.shape)\n",
    "            # print(\"dIdF3: \", dIdF3.shape)\n",
    "            # print(\"Q: \", Q.shape)\n",
    "            # print(\"grad_Q: \", grad_Q.shape)\n",
    "            # print(\"dQbdI1: \", dQbdI1.shape)\n",
    "            # print(\"dQbdI2: \", dQbdI2.shape)\n",
    "            # print(\"dQbdI3: \", dQbdI3.shape)\n",
    "            # print(\"P: \", P.shape)\n",
    "            \n",
    "            \n",
    "            # print(\"grad_P: \", grad_P[0].shape)\n",
    "            # print(\"Pxx: \", Pxx.shape)\n",
    "            # print(\"Pxy: \", Pxy.shape)\n",
    "            # print(\"Pyx: \", Pyx.shape)\n",
    "            # print(\"Pyy: \", Pyy.shape)\n",
    "            # print(\"omega_X: \", omega_X.shape)\n",
    "            # print(\"omega_Y: \", omega_Y.shape)\n",
    "            # print(\"dPxxdx: \", dPxxdx.shape)\n",
    "            \n",
    "            # Regularization term\n",
    "            square_params, num = 0.0, 0\n",
    "            for param in self.parameters():\n",
    "                num += 1\n",
    "                square_params += torch.norm(param)**2  # L2 norm of parameters\n",
    "            # square_weights_sum = 0\n",
    "            # for layer in self._linears:\n",
    "            #     square_weights_sum += torch.square(layer.weight).sum()\n",
    "            # square_beta_sum = self._beta.sum()\n",
    "            # regularized_params = square_weights_sum + square_beta_sum\n",
    "                \n",
    "            # Experimental Loss on Interior Points\n",
    "            loss_exp = self._loss_function(u_train, u_train_hat)\n",
    "            \n",
    "            # Boundary Condition Loss\n",
    "            loss_bc = torch.tensor(0.0).reshape(1)\n",
    "            r_train_mod = torch.absolute(r_train)\n",
    "            # print(\"r_train_mod: \", r_train_mod)\n",
    "            if loc == 0: # South Boundary\n",
    "                x_coord = omega_train[:, 0]\n",
    "                piola_stress22 = P[:, 3]\n",
    "                piola_stress12 = P[:, 1]\n",
    "                resultant22 = (torch.trapz(piola_stress22, x_coord)).reshape(1)\n",
    "                resultant12 = (torch.trapz(piola_stress12, x_coord)).reshape(1)\n",
    "                loss_bc += (resultant22-r_train_mod[0])**2 + resultant12**2\n",
    "                # print(\"piola_stress: \", piola_stress.shape, x_coord.shape)\n",
    "                # print(\"resultant: \", resultant.shape)\n",
    "                # print(\"r_train_mod[0]: \", r_train_mod[0].shape)\n",
    "                # print(loss_bc.shape)\n",
    "            elif loc == 1: # North Boundary\n",
    "                x_coord = omega_train[:, 0]\n",
    "                piola_stress22 = P[:, 3]\n",
    "                piola_stress12 = P[:, 1]\n",
    "                resultant22 = (torch.trapz(piola_stress22, x_coord)).reshape(1)\n",
    "                resultant12 = (torch.trapz(piola_stress12, x_coord)).reshape(1)\n",
    "                loss_bc += (resultant22-r_train_mod[1])**2 + resultant12**2\n",
    "            elif loc == 2: # West Boundary\n",
    "                y_coord = omega_train[:, 1]\n",
    "                piola_stress11= P[:, 0]\n",
    "                piola_stress21 = P[:, 2]\n",
    "                resultant11 = (torch.trapz(piola_stress11, y_coord)).reshape(1)\n",
    "                resultant21 = (torch.trapz(piola_stress21, y_coord)).reshape(1)\n",
    "                loss_bc += (resultant11-r_train_mod[2])**2 + resultant21**2\n",
    "            elif loc == 3:\n",
    "                y_coord = omega_train[:, 1]\n",
    "                piola_stress11= P[:, 0]\n",
    "                piola_stress21 = P[:, 2]\n",
    "                resultant11 = (torch.trapz(piola_stress11, y_coord)).reshape(1)\n",
    "                resultant21 = (torch.trapz(piola_stress21, y_coord)).reshape(1)\n",
    "                loss_bc += (resultant11-r_train_mod[3])**2 + resultant21**2\n",
    "            \n",
    "            # PDE Loss\n",
    "            loss_pde = torch.tensor(0.0) # this loss was turning out to be very small, and the evaluation was very costly\n",
    "            \n",
    "            hp1, hp2, hp3 = self._hyperparams\n",
    "            # total_loss = loss_pde + hp1 * loss_exp + hp2 * loss_bc + hp3 * square_weights_sum\n",
    "            \n",
    "            total_loss = loss_pde + hp1 * loss_exp + hp2 * loss_bc + hp3 * square_params\n",
    "            \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============Epoch==========> 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Loss: 0.3346674144268036\n",
      "Batch: 1, Loss: 0.30927908420562744\n",
      "Batch: 2, Loss: 0.3156856298446655\n",
      "Batch: 3, Loss: 0.26928839087486267\n",
      "Batch: 4, Loss: 0.2694980800151825\n",
      "Batch: 5, Loss: 0.25597715377807617\n",
      "Batch: 6, Loss: 0.24758559465408325\n",
      "Batch: 7, Loss: 0.24115781486034393\n",
      "Batch: 8, Loss: 0.21475300192832947\n",
      "Batch: 9, Loss: 0.21606573462486267\n",
      "Batch: 10, Loss: 0.2014317512512207\n",
      "Batch: 11, Loss: 0.19223910570144653\n",
      "Batch: 12, Loss: 0.17780587077140808\n",
      "Batch: 13, Loss: 0.16942360997200012\n",
      "Batch: 14, Loss: 0.16500872373580933\n",
      "Batch: 15, Loss: 0.15341505408287048\n",
      "Batch: 16, Loss: 0.15160208940505981\n",
      "Batch: 17, Loss: 0.14301897585391998\n",
      "Batch: 18, Loss: 0.13862664997577667\n",
      "Batch: 19, Loss: 0.1289423704147339\n",
      "Batch: 20, Loss: 0.12543326616287231\n",
      "Batch: 21, Loss: 0.1183229386806488\n",
      "Batch: 22, Loss: 0.11405692249536514\n",
      "Batch: 23, Loss: 0.10914276540279388\n",
      "Batch: 24, Loss: 0.10815595090389252\n",
      "===============Epoch==========> 1\n",
      "Batch: 0, Loss: 0.1090879738330841\n",
      "Batch: 1, Loss: 0.09941528737545013\n",
      "Batch: 2, Loss: 0.10660247504711151\n",
      "Batch: 3, Loss: 0.09231901168823242\n",
      "Batch: 4, Loss: 0.09215448796749115\n",
      "Batch: 5, Loss: 0.0891101285815239\n",
      "Batch: 6, Loss: 0.08884212374687195\n",
      "Batch: 7, Loss: 0.08884904533624649\n",
      "Batch: 8, Loss: 0.08502183854579926\n",
      "Batch: 9, Loss: 0.08650924265384674\n",
      "Batch: 10, Loss: 0.08500118553638458\n",
      "Batch: 11, Loss: 0.0847146138548851\n",
      "Batch: 12, Loss: 0.08409575372934341\n",
      "Batch: 13, Loss: 0.08391851931810379\n",
      "Batch: 14, Loss: 0.08432174474000931\n",
      "Batch: 15, Loss: 0.08438225835561752\n",
      "Batch: 16, Loss: 0.08402666449546814\n",
      "Batch: 17, Loss: 0.08397164940834045\n",
      "Batch: 18, Loss: 0.0840168297290802\n",
      "Batch: 19, Loss: 0.08402124047279358\n",
      "Batch: 20, Loss: 0.08389079570770264\n",
      "Batch: 21, Loss: 0.08399344235658646\n",
      "Batch: 22, Loss: 0.08380074799060822\n",
      "Batch: 23, Loss: 0.08423130959272385\n",
      "Batch: 24, Loss: 0.08345899730920792\n",
      "===============Epoch==========> 2\n",
      "Batch: 0, Loss: 0.08524735271930695\n",
      "Batch: 1, Loss: 0.08730721473693848\n",
      "Batch: 2, Loss: 0.0854186937212944\n",
      "Batch: 3, Loss: 0.0882968083024025\n",
      "Batch: 4, Loss: 0.08352784067392349\n",
      "Batch: 5, Loss: 0.08299772441387177\n",
      "Batch: 6, Loss: 0.082362599670887\n",
      "Batch: 7, Loss: 0.08196030557155609\n",
      "Batch: 8, Loss: 0.0830799862742424\n",
      "Batch: 9, Loss: 0.08173023164272308\n",
      "Batch: 10, Loss: 0.08190938830375671\n",
      "Batch: 11, Loss: 0.08175679296255112\n",
      "Batch: 12, Loss: 0.08187545090913773\n",
      "Batch: 13, Loss: 0.08159300684928894\n",
      "Batch: 14, Loss: 0.08142910897731781\n",
      "Batch: 15, Loss: 0.0814986303448677\n",
      "Batch: 16, Loss: 0.08102010935544968\n",
      "Batch: 17, Loss: 0.0808710977435112\n",
      "Batch: 18, Loss: 0.08091156929731369\n",
      "Batch: 19, Loss: 0.08058905601501465\n",
      "Batch: 20, Loss: 0.08063548803329468\n",
      "Batch: 21, Loss: 0.08044538646936417\n",
      "Batch: 22, Loss: 0.0802789181470871\n",
      "Batch: 23, Loss: 0.08050402253866196\n",
      "Batch: 24, Loss: 0.08063717186450958\n",
      "Training Time:  1729.2218186855316\n"
     ]
    }
   ],
   "source": [
    "layers = [2,50,10,2]\n",
    "model = Physics_Informed_NN(layers=layers, num_features=43)\n",
    "# model.loss_criterion(u_train=u_int[20], r_train=reaction, omega_train=X_int[20])\n",
    "\n",
    "BATCH = 25\n",
    "MAX_EPOCHS = 3\n",
    "\n",
    "# optimizer = optim.LBFGS(model.parameters(), lr=0.1, \n",
    "#                               max_iter=5,  \n",
    "#                               max_eval = None, \n",
    "#                               tolerance_grad = 1e-06, \n",
    "#                               tolerance_change = 1e-09, \n",
    "#                               history_size = 100, \n",
    "#                               line_search_fn = 'strong_wolfe')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    print(\"===============Epoch==========>\", epoch)\n",
    "    # interior_index, boundary_index = 0, 0\n",
    "    for batch in range(BATCH):\n",
    "        # print(f'Batch: {batch}')\n",
    "        if batch <= 3:\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                loss = model.loss_criterion(u_train=u_bound[batch], r_train=reaction, omega_train=X_bound[batch], loc=batch)\n",
    "                # boundary_index += 1\n",
    "                loss.backward()\n",
    "                print(f'Batch: {batch}, Loss: {loss.item()}') # Batch: {batch}, \n",
    "                return loss\n",
    "            optimizer.step(closure)\n",
    "        else:\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                loss = model.loss_criterion(u_train=u_int[batch - 4], r_train=reaction, omega_train=X_int[batch - 4])\n",
    "                # interior_index += 1\n",
    "                print(f'Batch: {batch}, Loss: {loss.item()}')\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            optimizer.step(closure)\n",
    "            \n",
    "print('Training Time: ', time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The beta parameters of the model are, \n",
      "Index: 36, Beta: -0.028982507064938545\n",
      "Index: 2, Beta: -0.019829921424388885\n",
      "Index: 43, Beta: -0.018234340474009514\n",
      "Index: 1, Beta: -0.01777459867298603\n",
      "Index: 5, Beta: -0.016955263912677765\n",
      "Index: 4, Beta: -0.015920501202344894\n",
      "Index: 3, Beta: -0.014717147685587406\n",
      "Index: 37, Beta: -0.012779138050973415\n",
      "Index: 9, Beta: -0.008368752896785736\n",
      "Index: 8, Beta: -0.007433408405631781\n",
      "Index: 7, Beta: -0.006407493259757757\n",
      "Index: 6, Beta: -0.005308269057422876\n",
      "Index: 20, Beta: 0.0007336471462622285\n",
      "Index: 19, Beta: 0.0004912199219688773\n",
      "Index: 16, Beta: -0.00043488582014106214\n",
      "Index: 17, Beta: -0.00031879753805696964\n",
      "Index: 15, Beta: -0.00027281633811071515\n",
      "Index: 14, Beta: -0.000212154453038238\n",
      "Index: 25, Beta: 0.00018640853522811085\n",
      "Index: 26, Beta: 0.000160772746312432\n",
      "Index: 10, Beta: -0.00014509752509184182\n",
      "Index: 22, Beta: -0.0001206896995427087\n",
      "Index: 23, Beta: -9.17998404474929e-05\n",
      "Index: 11, Beta: -6.934937846381217e-05\n",
      "Index: 24, Beta: 6.765976286260411e-05\n",
      "Index: 39, Beta: 6.598136678803712e-05\n",
      "Index: 18, Beta: 6.443208258133382e-05\n",
      "Index: 38, Beta: 5.8024117606692016e-05\n",
      "Index: 33, Beta: -5.0278766138944775e-05\n",
      "Index: 27, Beta: 4.8074580263346434e-05\n",
      "Index: 34, Beta: -3.3114854886662215e-05\n",
      "Index: 21, Beta: -2.936880264314823e-05\n",
      "Index: 13, Beta: -2.7675909223034978e-05\n",
      "Index: 35, Beta: 2.4329838197445497e-05\n",
      "Index: 32, Beta: -2.3022670575301163e-05\n",
      "Index: 30, Beta: 1.9193486878066324e-05\n",
      "Index: 29, Beta: 1.9032911950489506e-05\n",
      "Index: 28, Beta: 1.5279767467291094e-05\n",
      "Index: 42, Beta: 7.1218869379663374e-06\n",
      "Index: 31, Beta: 6.952626790734939e-06\n",
      "Index: 41, Beta: -6.922108696016949e-06\n",
      "Index: 40, Beta: 4.654953954741359e-06\n",
      "Index: 12, Beta: -1.8227947293780744e-06\n"
     ]
    }
   ],
   "source": [
    "beta_params = model._beta\n",
    "\n",
    "# Get the indices for each value\n",
    "indices = torch.arange(beta_params.size(0))\n",
    "\n",
    "# Sort the absolute values of beta_params tensor and indices in descending order\n",
    "sorted_indices = torch.argsort(torch.abs(beta_params.squeeze()), descending=True)\n",
    "sorted_beta_params = beta_params[sorted_indices]\n",
    "\n",
    "# Print the corresponding indices and beta values\n",
    "print(\"The beta parameters of the model are, \")\n",
    "for i, beta in zip(sorted_indices, sorted_beta_params):\n",
    "    print(f\"Index: {i.item()+1}, Beta: {beta.item()}\")\n",
    "\n",
    "# for i, beta in enumerate(model._beta):\n",
    "#     print(f'{i+1} ====> {beta.item()}')\n",
    "# print(model._beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
